{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n%pip install -U transformers datasets accelerate peft trl bitsandbytes wandb gradio","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-06T16:29:15.639747Z","iopub.execute_input":"2025-04-06T16:29:15.640062Z","iopub.status.idle":"2025-04-06T16:29:52.847574Z","shell.execute_reply.started":"2025-04-06T16:29:15.640034Z","shell.execute_reply":"2025-04-06T16:29:52.846635Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    logging,\n)\nfrom peft import (\n    LoraConfig,\n    get_peft_model,\n)\nimport os, torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T16:29:59.318817Z","iopub.execute_input":"2025-04-06T16:29:59.319121Z","iopub.status.idle":"2025-04-06T16:30:24.453335Z","shell.execute_reply.started":"2025-04-06T16:29:59.319098Z","shell.execute_reply":"2025-04-06T16:30:24.452673Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Load Hugging Face and Weights & Biases tokens\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nwb_token = user_secrets.get_secret(\"wandb\")\n\nlogin(token=hf_token)\nwandb.login(key=wb_token)\n\nrun = wandb.init(\n    project='Fine-tune Gemma-2- 2B on MMLU', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T16:30:28.951026Z","iopub.execute_input":"2025-04-06T16:30:28.951901Z","iopub.status.idle":"2025-04-06T16:30:41.762986Z","shell.execute_reply.started":"2025-04-06T16:30:28.951870Z","shell.execute_reply":"2025-04-06T16:30:41.762355Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33muu0712\u001b[0m (\u001b[33muu0712-engineering-student-council\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250406_163035-kq3r2eif</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/uu0712-engineering-student-council/Fine-tune%20Gemma-2-%202B%20on%20MMLU/runs/kq3r2eif?apiKey=d54064a15224874b6726f57b6ea6f9eb29a18c63' target=\"_blank\">defiant-tanagra-12</a></strong> to <a href='https://wandb.ai/uu0712-engineering-student-council/Fine-tune%20Gemma-2-%202B%20on%20MMLU?apiKey=d54064a15224874b6726f57b6ea6f9eb29a18c63' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/uu0712-engineering-student-council/Fine-tune%20Gemma-2-%202B%20on%20MMLU?apiKey=d54064a15224874b6726f57b6ea6f9eb29a18c63' target=\"_blank\">https://wandb.ai/uu0712-engineering-student-council/Fine-tune%20Gemma-2-%202B%20on%20MMLU?apiKey=d54064a15224874b6726f57b6ea6f9eb29a18c63</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/uu0712-engineering-student-council/Fine-tune%20Gemma-2-%202B%20on%20MMLU/runs/kq3r2eif?apiKey=d54064a15224874b6726f57b6ea6f9eb29a18c63' target=\"_blank\">https://wandb.ai/uu0712-engineering-student-council/Fine-tune%20Gemma-2-%202B%20on%20MMLU/runs/kq3r2eif?apiKey=d54064a15224874b6726f57b6ea6f9eb29a18c63</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Do NOT share these links with anyone. They can be used to claim your runs."},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"base_model = \"google/gemma-2-2b-it\"\ndataset = \"openai/gsm8k\"# Updated to cais/mmlu\ndataset2 = \"openai/gsm8k\"\nnew_model = \"gemma-2b-mmlu-pro-openai/gsm8k\"\n\ntorch_dtype = torch.float16\nattn_implementation = \"eager\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T16:30:44.482687Z","iopub.execute_input":"2025-04-06T16:30:44.483001Z","iopub.status.idle":"2025-04-06T16:30:44.487923Z","shell.execute_reply.started":"2025-04-06T16:30:44.482967Z","shell.execute_reply":"2025-04-06T16:30:44.487053Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# QLoRA config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch_dtype,\n    bnb_4bit_use_double_quant=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T16:30:47.478559Z","iopub.execute_input":"2025-04-06T16:30:47.478856Z","iopub.status.idle":"2025-04-06T16:30:47.484306Z","shell.execute_reply.started":"2025-04-06T16:30:47.478833Z","shell.execute_reply":"2025-04-06T16:30:47.483465Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    attn_implementation=attn_implementation\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T04:08:46.663932Z","iopub.execute_input":"2025-04-05T04:08:46.664253Z","iopub.status.idle":"2025-04-05T04:09:11.924772Z","shell.execute_reply.started":"2025-04-05T04:08:46.664225Z","shell.execute_reply":"2025-04-05T04:09:11.923900Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/838 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bafd857ecb47448e9060529230a4f479"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2065bb814ac1473e90173c169596bc0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d1d2b717e834fd7908db3149874150e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85e86f301c8f43b3924d5423c0740aaf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c08276cdbd54263b8487581fd7da9c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d9a748abc944f69b0af3ed8732c30ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30e40e80f51747afb4e19cb5f74010d9"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"print(dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T17:44:46.535876Z","iopub.execute_input":"2025-04-04T17:44:46.536194Z","iopub.status.idle":"2025-04-04T17:44:46.541324Z","shell.execute_reply.started":"2025-04-04T17:44:46.536167Z","shell.execute_reply":"2025-04-04T17:44:46.540494Z"}},"outputs":[{"name":"stdout","text":"openai/gsm8k\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoTokenizer\n\n# 🔹 Step 1: Load the dataset\ndataset = load_dataset(\"openai/gsm8k\", \"main\")\n\n# 🔹 Step 2: Format the prompt (Question-Answer pair)\ndef format_prompt(example):\n    return {\n        \"text\": f\"Question: {example['question']}\\nAnswer: {example['answer']}\"\n    }\n\nformatted_dataset = dataset[\"train\"].map(format_prompt)\n\n# 🔹 Step 3: Load the Gemma tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\ntokenizer.pad_token = tokenizer.eos_token  # Set EOS token as PAD token\n\n# 🔹 Step 4: Tokenize the prompts\ndef tokenize(example):\n    return tokenizer(example[\"text\"], truncation=True, padding=False)\n\ntokenized_dataset = formatted_dataset.map(tokenize, remove_columns=formatted_dataset.column_names)\n\n# ✅ Now tokenized_dataset is ready for fine-tuning\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T17:50:02.197972Z","iopub.execute_input":"2025-04-04T17:50:02.198425Z","iopub.status.idle":"2025-04-04T17:50:13.161136Z","shell.execute_reply.started":"2025-04-04T17:50:02.198382Z","shell.execute_reply":"2025-04-04T17:50:13.160120Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6908412f492486795f4a5769e718cb1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/2.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c2cbd8faa0c476e9ccb68fea833d132"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/419k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e465d320c9c54b39a26d712ac2f161a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99dac9f24fa046128587859531812b5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ebaacd90e1740da8871780b8cebda45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7473 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea5ae33582834aa8a823f87e8c3ef0b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff685eca76d74ad69c5ee8dcdcc232f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af198c5203464030819fc6e03697af6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb8bfd645bc147d899c936a3d2a9fabe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f022e3e5768d49959fe88ea9af432170"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7473 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"911384d243f6411f8c2ed24a24789d35"}},"metadata":{}},{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"print(tokenized_dataset[0])\nprint(tokenizer.decode(tokenized_dataset[0][\"input_ids\"]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T17:50:13.162448Z","iopub.execute_input":"2025-04-04T17:50:13.162750Z","iopub.status.idle":"2025-04-04T17:50:13.170450Z","shell.execute_reply.started":"2025-04-04T17:50:13.162726Z","shell.execute_reply":"2025-04-04T17:50:13.169786Z"}},"outputs":[{"name":"stdout","text":"{'input_ids': [2, 9413, 235292, 101268, 7596, 35382, 577, 235248, 235310, 235321, 576, 1070, 4078, 575, 4623, 235269, 578, 1492, 1284, 7596, 3933, 685, 1767, 35382, 575, 2782, 235265, 2250, 1767, 35382, 1498, 101268, 4874, 29911, 575, 4623, 578, 2782, 235336, 108, 1261, 235292, 101268, 7596, 235248, 235310, 235321, 235283, 235284, 589, 3245, 235310, 235321, 235283, 235284, 235293, 235284, 235310, 2492, 235284, 235310, 35382, 575, 2782, 235265, 108, 140199, 7596, 235248, 235310, 235321, 235340, 235284, 235310, 589, 3245, 235310, 235321, 235340, 235284, 235310, 235293, 235324, 235284, 2492, 235324, 235284, 35382, 29911, 575, 4623, 578, 2782, 235265, 108, 3308, 235248, 235324, 235284], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n<bos>Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\nAnswer: Natalia sold 48/2 = <<48/2=24>>24 clips in May.\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n#### 72\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"print (dataset[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T17:35:01.962328Z","iopub.execute_input":"2025-04-04T17:35:01.962651Z","iopub.status.idle":"2025-04-04T17:35:01.968712Z","shell.execute_reply.started":"2025-04-04T17:35:01.962625Z","shell.execute_reply":"2025-04-04T17:35:01.967708Z"}},"outputs":[{"name":"stdout","text":"{'question': 'Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?', 'answer': 'Weng earns 12/60 = $<<12/60=0.2>>0.2 per minute.\\nWorking 50 minutes, she earned 0.2 x 50 = $<<0.2*50=10>>10.\\n#### 10'}\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# LoRA config\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=['q_proj', 'v_proj', 'k_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n)\n\nmodel = get_peft_model(model, peft_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T04:09:12.012814Z","iopub.execute_input":"2025-04-05T04:09:12.013133Z","iopub.status.idle":"2025-04-05T04:09:12.468195Z","shell.execute_reply.started":"2025-04-05T04:09:12.013106Z","shell.execute_reply":"2025-04-05T04:09:12.467325Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for seperate split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T10:34:40.218416Z","iopub.execute_input":"2025-03-04T10:34:40.218769Z","iopub.status.idle":"2025-03-04T10:34:40.222923Z","shell.execute_reply.started":"2025-03-04T10:34:40.218736Z","shell.execute_reply":"2025-03-04T10:34:40.222146Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import torch\nfrom transformers import TrainingArguments\nfrom trl import SFTTrainer\nimport wandb\n\n# Initialize wandb manually\nwandb.init(project=\"myv\", name=\"run_gemma_gsm8k_noeval\")\n\n# Define your model names\nbase_model = \"google/gemma-2b\"\nnew_model = \"gemma-2b-gsm8k-lora\"\n\n# Define training arguments\ntraining_arguments = TrainingArguments(\n    output_dir=new_model,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=2,\n    optim=\"paged_adamw_32bit\",\n    num_train_epochs=1,\n    logging_steps=50,\n    warmup_steps=100,\n    logging_strategy=\"steps\",\n    learning_rate=2e-4,\n    fp16=False,\n    bf16=False,\n    group_by_length=True,\n    report_to=\"wandb\",\n    save_strategy=\"epoch\",\n    save_total_limit=3,\n    run_name=\"gemma_gsm8k_noeval\",\n    evaluation_strategy=\"no\",   # ✅ Disable evaluation\n)\n\n# Start training\nprint(\"\\n🚀 Starting fine-tuning on full GSM8K dataset (no evaluation)...\\n\")\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=tokenized_dataset,  # Use full dataset for training\n    eval_dataset=None,                # ✅ No evaluation\n    peft_config=peft_config,\n    args=training_arguments,\n)\n\ntrainer.train()\n\n# Save final model\ntrainer.save_model(f\"{new_model}_final\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T17:50:25.185163Z","iopub.execute_input":"2025-04-04T17:50:25.185462Z","iopub.status.idle":"2025-04-04T18:57:29.547865Z","shell.execute_reply.started":"2025-04-04T17:50:25.185441Z","shell.execute_reply":"2025-04-04T18:57:29.547168Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">woven-violet-2</strong> at: <a href='https://wandb.ai/uu0712-engineering-student-council/Fine-tune%20Gemma-2-%202B%20on%20MMLU/runs/p7ncvvw2?apiKey=d54064a15224874b6726f57b6ea6f9eb29a18c63' target=\"_blank\">https://wandb.ai/uu0712-engineering-student-council/Fine-tune%20Gemma-2-%202B%20on%20MMLU/runs/p7ncvvw2?apiKey=d54064a15224874b6726f57b6ea6f9eb29a18c63</a><br> View project at: <a href='https://wandb.ai/uu0712-engineering-student-council/Fine-tune%20Gemma-2-%202B%20on%20MMLU?apiKey=d54064a15224874b6726f57b6ea6f9eb29a18c63' target=\"_blank\">https://wandb.ai/uu0712-engineering-student-council/Fine-tune%20Gemma-2-%202B%20on%20MMLU?apiKey=d54064a15224874b6726f57b6ea6f9eb29a18c63</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250404_174913-p7ncvvw2/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250404_175025-gzetp0hk</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/uu0712-engineering-student-council/myv/runs/gzetp0hk?apiKey=d54064a15224874b6726f57b6ea6f9eb29a18c63' target=\"_blank\">run_gemma_gsm8k_noeval</a></strong> to <a href='https://wandb.ai/uu0712-engineering-student-council/myv?apiKey=d54064a15224874b6726f57b6ea6f9eb29a18c63' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/uu0712-engineering-student-council/myv?apiKey=d54064a15224874b6726f57b6ea6f9eb29a18c63' target=\"_blank\">https://wandb.ai/uu0712-engineering-student-council/myv?apiKey=d54064a15224874b6726f57b6ea6f9eb29a18c63</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/uu0712-engineering-student-council/myv/runs/gzetp0hk?apiKey=d54064a15224874b6726f57b6ea6f9eb29a18c63' target=\"_blank\">https://wandb.ai/uu0712-engineering-student-council/myv/runs/gzetp0hk?apiKey=d54064a15224874b6726f57b6ea6f9eb29a18c63</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Do NOT share these links with anyone. They can be used to claim your runs."},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n🚀 Starting fine-tuning on full GSM8K dataset (no evaluation)...\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/7473 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9aa575866d249d19ceb77a15e1d8f78"}},"metadata":{}},{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3736' max='3736' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3736/3736 1:06:50, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>1.109100</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.945100</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.910100</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.954500</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.907100</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.957900</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.924400</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.917900</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.913500</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.908000</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.894500</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.896300</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.900000</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.857900</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.825600</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.910700</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.894200</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.853800</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.889100</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.875500</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.892400</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.852500</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.839100</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.889500</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.867100</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.835100</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.905100</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>0.844400</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.875800</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>0.897900</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.892000</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>0.851800</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.904300</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>0.862800</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.876100</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>0.869900</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.869300</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>0.846900</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.849400</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>0.866100</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.895100</td>\n    </tr>\n    <tr>\n      <td>2150</td>\n      <td>0.873000</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.831000</td>\n    </tr>\n    <tr>\n      <td>2250</td>\n      <td>0.804000</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.837700</td>\n    </tr>\n    <tr>\n      <td>2350</td>\n      <td>0.853400</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.818900</td>\n    </tr>\n    <tr>\n      <td>2450</td>\n      <td>0.850700</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.847500</td>\n    </tr>\n    <tr>\n      <td>2550</td>\n      <td>0.852400</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.866100</td>\n    </tr>\n    <tr>\n      <td>2650</td>\n      <td>0.830400</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.838400</td>\n    </tr>\n    <tr>\n      <td>2750</td>\n      <td>0.804800</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.829600</td>\n    </tr>\n    <tr>\n      <td>2850</td>\n      <td>0.850200</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.830200</td>\n    </tr>\n    <tr>\n      <td>2950</td>\n      <td>0.826700</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.839400</td>\n    </tr>\n    <tr>\n      <td>3050</td>\n      <td>0.888300</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>0.837900</td>\n    </tr>\n    <tr>\n      <td>3150</td>\n      <td>0.816800</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>0.822600</td>\n    </tr>\n    <tr>\n      <td>3250</td>\n      <td>0.842000</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>0.837300</td>\n    </tr>\n    <tr>\n      <td>3350</td>\n      <td>0.826800</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>0.839000</td>\n    </tr>\n    <tr>\n      <td>3450</td>\n      <td>0.798500</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.843900</td>\n    </tr>\n    <tr>\n      <td>3550</td>\n      <td>0.822500</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>0.823000</td>\n    </tr>\n    <tr>\n      <td>3650</td>\n      <td>0.833800</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>0.811600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"import torch\n\n# Save the model and tokenizer\nmodel.save_pretrained(\"gemma-2b-gsm8k-finetuned\")\ntokenizer.save_pretrained(\"gemma-2b-gsm8k-finetuned\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:58:38.421411Z","iopub.execute_input":"2025-04-04T18:58:38.421750Z","iopub.status.idle":"2025-04-04T18:58:39.388133Z","shell.execute_reply.started":"2025-04-04T18:58:38.421723Z","shell.execute_reply":"2025-04-04T18:58:39.387243Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"('gemma-2b-gsm8k-finetuned/tokenizer_config.json',\n 'gemma-2b-gsm8k-finetuned/special_tokens_map.json',\n 'gemma-2b-gsm8k-finetuned/tokenizer.model',\n 'gemma-2b-gsm8k-finetuned/added_tokens.json',\n 'gemma-2b-gsm8k-finetuned/tokenizer.json')"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\n# Load the fine-tuned model\nmodel_path = \"gemma-2b-gsm8k-finetuned\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16).to(\"cuda\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:58:59.217832Z","iopub.execute_input":"2025-04-04T18:58:59.218184Z","iopub.status.idle":"2025-04-04T18:59:08.368996Z","shell.execute_reply.started":"2025-04-04T18:58:59.218154Z","shell.execute_reply":"2025-04-04T18:59:08.368279Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a937d67d26584e4cb4395399ab2b9b08"}},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"def generate_answer(question):\n    prompt = f\"Question: {question}\\nAnswer:\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n    output = model.generate(**inputs, max_length=200)\n    return tokenizer.decode(output[0], skip_special_tokens=True)\n\n# Example Question\ntest_question = \"If a train travels 120 miles in 2 hours, what is its average speed?\"\ngenerated_answer = generate_answer(test_question)\n\nprint(\"🔹 Model's Answer:\\n\", generated_answer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:59:22.298463Z","iopub.execute_input":"2025-04-04T18:59:22.298787Z","iopub.status.idle":"2025-04-04T18:59:34.957094Z","shell.execute_reply.started":"2025-04-04T18:59:22.298758Z","shell.execute_reply":"2025-04-04T18:59:34.956315Z"}},"outputs":[{"name":"stdout","text":"🔹 Model's Answer:\n Question: If a train travels 120 miles in 2 hours, what is its average speed?\nAnswer: The train travels 120 miles / 2 hours = <<120/2=60>>60 miles per hour.\n#### 60 miles per hour\n#### 60 mph\n#### 60 mph\n#### 60 mph\n#### 60 mph\n#### 60 mph\n#### 60 mph\n#### 60 mph\n#### 60 mph\n#### 60 mph\n#### 60 mph\n#### 60 mph\n#### 60 mph\n#### 60 mph\n#### 60 mph\n#### 60 mph\n#### 60 mph\n#### 60 mph\n#### 60 mph\n#### 60 mph\n#### 60 mph\n#### 60 mph\n#### 60 mph\n#### 60\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"from huggingface_hub import create_repo\n\nrepo_name = \"Working-gemma-2-2b-it-gsm8k\"  # Change this if needed\ncreate_repo(repo_name, repo_type=\"model\", exist_ok=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T19:02:09.990656Z","iopub.execute_input":"2025-04-04T19:02:09.990977Z","iopub.status.idle":"2025-04-04T19:02:24.778264Z","shell.execute_reply.started":"2025-04-04T19:02:09.990952Z","shell.execute_reply":"2025-04-04T19:02:24.777463Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"RepoUrl('https://huggingface.co/sparky353454/Working-gemma-2-2b-it-gsm8k', endpoint='https://huggingface.co', repo_type='model', repo_id='sparky353454/Working-gemma-2-2b-it-gsm8k')"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_path = \"/kaggle/working/gemma-2b-gsm8k-finetuned\"  # Your saved model path\n\n# Load the fine-tuned model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# Push to Hugging Face Hub\nmodel.push_to_hub(\"sparky353454/gemma-2b-gsm8k\")\ntokenizer.push_to_hub(\"sparky353454/gemma-2b-gsm8k\")\nprint(\"completed \")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T19:04:46.082345Z","iopub.execute_input":"2025-04-04T19:04:46.082665Z","iopub.status.idle":"2025-04-04T19:05:06.380149Z","shell.execute_reply.started":"2025-04-04T19:04:46.082642Z","shell.execute_reply":"2025-04-04T19:05:06.379350Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a275b1c3274449dda45d85c828ea2c8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/83.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"826bd8ed732f44d1b76966ef4b55eab7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"239d0cde1cc6440faa4e57f551c1f8d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5f14865d7ba4633b3e9170ee8574577"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70428cb4955145b68b85dd154245b7d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/34.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72c12643c89d44c59aa773b96bae2808"}},"metadata":{}},{"name":"stdout","text":"completed \n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"!pip install gradio","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-06T16:28:53.053Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport gradio as gr\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Load the fine-tuned model from Hugging Face\nmodel_name = \"sparky353454/gemma-2b-gsm8k\"  \ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n\n# Function to generate response\ndef generate_answer(question):\n    prompt = f\"Question: {question}\\nAnswer:\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n    with torch.no_grad():\n        output = model.generate(**inputs, max_new_tokens=200)\n    \n    answer = tokenizer.decode(output[0], skip_special_tokens=True)\n    return answer.split(\"Answer:\")[-1].strip()\n\n# Create Gradio UI\niface = gr.Interface(\n    fn=generate_answer,\n    inputs=gr.Textbox(label=\"Enter Your Math Question\"),\n    outputs=gr.Textbox(label=\"Model's Answer\"),\n    title=\"Gemma 2B - GSM8K Math Solver\",\n    description=\"Enter a mathematical reasoning question, and the fine-tuned Gemma 2B model will generate the answer.\"\n)\n\n# Launch the app\niface.launch()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T04:10:12.494886Z","iopub.execute_input":"2025-04-05T04:10:12.495202Z","iopub.status.idle":"2025-04-05T04:10:23.828974Z","shell.execute_reply.started":"2025-04-05T04:10:12.495177Z","shell.execute_reply":"2025-04-05T04:10:23.828320Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7705f716d82d43d082dce514f17e2481"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16bd3b450cbb458f98002d17e76d6ae2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/34.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0ce2b38df9a4d23a3b2262998a87f4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7d7954d4ebc42eb9f2bfdbe41b25cf5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/853 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b916a3cb6bd9466fa34c288e6fdd5a25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f030d670564d41fb9b8aeaa26b27777a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/83.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdf42521e5094291ae890a7d2ac2646a"}},"metadata":{}},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\nKaggle notebooks require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://2700bf3668f2c8378b.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://2700bf3668f2c8378b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"import getpass\n\n# Prompt for GitHub token\ntoken = getpass.getpass(\"Enter your GitHub token (starts with 'ghp_' or 'github_pat_...'): \")\n\n# Use EleutherAI repo instead of OpenAI\nrepo_url = f\"https://{token}:x-oauth-basic@github.com/EleutherAI/lm-evaluation-harness.git\"\n\n# Clone it\n!git clone {repo_url}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T13:16:32.179311Z","iopub.execute_input":"2025-04-06T13:16:32.179661Z","iopub.status.idle":"2025-04-06T13:16:42.200278Z","shell.execute_reply.started":"2025-04-06T13:16:32.179639Z","shell.execute_reply":"2025-04-06T13:16:42.199351Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Enter your GitHub token (starts with 'ghp_' or 'github_pat_...'):  ········\n"},{"name":"stdout","text":"Cloning into 'lm-evaluation-harness'...\nremote: Enumerating objects: 49581, done.\u001b[K\nremote: Counting objects: 100% (18/18), done.\u001b[K\nremote: Compressing objects: 100% (16/16), done.\u001b[K\nremote: Total 49581 (delta 10), reused 2 (delta 2), pack-reused 49563 (from 2)\u001b[K\nReceiving objects: 100% (49581/49581), 29.58 MiB | 22.00 MiB/s, done.\nResolving deltas: 100% (34299/34299), done.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"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\n\n\n#git token","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd lm-evaluation-harness\n!pip install -e .\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T13:16:48.193366Z","iopub.execute_input":"2025-04-06T13:16:48.193685Z","iopub.status.idle":"2025-04-06T13:17:08.896590Z","shell.execute_reply.started":"2025-04-06T13:16:48.193664Z","shell.execute_reply":"2025-04-06T13:17:08.895841Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/lm-evaluation-harness\nObtaining file:///kaggle/working/lm-evaluation-harness\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.8) (1.6.0)\nCollecting evaluate (from lm_eval==0.4.8)\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.8) (3.5.0)\nCollecting jsonlines (from lm_eval==0.4.8)\n  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.8) (2.10.2)\nRequirement already satisfied: peft>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.8) (0.15.1)\nRequirement already satisfied: pybind11>=2.6.2 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.8) (2.13.6)\nCollecting pytablewriter (from lm_eval==0.4.8)\n  Downloading pytablewriter-1.2.1-py3-none-any.whl.metadata (38 kB)\nCollecting rouge-score>=0.0.4 (from lm_eval==0.4.8)\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting sacrebleu>=1.5.0 (from lm_eval==0.4.8)\n  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.8) (1.2.2)\nCollecting sqlitedict (from lm_eval==0.4.8)\n  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: torch>=1.8 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.8) (2.5.1+cu121)\nCollecting tqdm-multiprocess (from lm_eval==0.4.8)\n  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl.metadata (5.7 kB)\nRequirement already satisfied: transformers>=4.1 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.8) (4.51.0)\nCollecting zstandard (from lm_eval==0.4.8)\n  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\nRequirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.8) (0.3.8)\nCollecting word2number (from lm_eval==0.4.8)\n  Downloading word2number-1.1.zip (9.7 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: more_itertools in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.8) (10.5.0)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.8) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.8) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.8) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.8) (6.0.2)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.8) (0.30.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.8) (0.4.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm_eval==0.4.8) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm_eval==0.4.8) (19.0.1)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm_eval==0.4.8) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm_eval==0.4.8) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm_eval==0.4.8) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm_eval==0.4.8) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm_eval==0.4.8) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.8) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm_eval==0.4.8) (3.11.12)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm_eval==0.4.8) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm_eval==0.4.8) (3.2.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm_eval==0.4.8) (1.17.0)\nCollecting portalocker (from sacrebleu>=1.5.0->lm_eval==0.4.8)\n  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.8) (2024.11.6)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.8) (0.9.0)\nRequirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.8) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.8) (5.3.0)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.4.8) (1.13.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.4.8) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.4.8) (3.5.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==0.4.8) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==0.4.8) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==0.4.8) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==0.4.8) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8->lm_eval==0.4.8) (1.3.0)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.1->lm_eval==0.4.8) (0.21.0)\nRequirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines->lm_eval==0.4.8) (25.1.0)\nRequirement already satisfied: setuptools>=38.3.0 in /usr/local/lib/python3.10/dist-packages (from pytablewriter->lm_eval==0.4.8) (75.1.0)\nCollecting DataProperty<2,>=1.1.0 (from pytablewriter->lm_eval==0.4.8)\n  Downloading DataProperty-1.1.0-py3-none-any.whl.metadata (11 kB)\nCollecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm_eval==0.4.8)\n  Downloading mbstrdecoder-1.1.4-py3-none-any.whl.metadata (4.3 kB)\nCollecting pathvalidate<4,>=2.3.0 (from pytablewriter->lm_eval==0.4.8)\n  Downloading pathvalidate-3.2.3-py3-none-any.whl.metadata (12 kB)\nCollecting tabledata<2,>=1.3.1 (from pytablewriter->lm_eval==0.4.8)\n  Downloading tabledata-1.3.4-py3-none-any.whl.metadata (3.7 kB)\nCollecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm_eval==0.4.8)\n  Downloading tcolorpy-0.1.7-py3-none-any.whl.metadata (6.3 kB)\nCollecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.8)\n  Downloading typepy-1.3.4-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.8) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.8) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.8) (5.0.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.8) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.8) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.8) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.8) (1.18.3)\nRequirement already satisfied: chardet<6,>=3.0.4 in /usr/local/lib/python3.10/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm_eval==0.4.8) (5.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.26.0->lm_eval==0.4.8) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.26.0->lm_eval==0.4.8) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.26.0->lm_eval==0.4.8) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.26.0->lm_eval==0.4.8) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.26.0->lm_eval==0.4.8) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.26.0->lm_eval==0.4.8) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->lm_eval==0.4.8) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->lm_eval==0.4.8) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->lm_eval==0.4.8) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->lm_eval==0.4.8) (2025.1.31)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.8) (2.9.0.post0)\nRequirement already satisfied: pytz>=2018.9 in /usr/local/lib/python3.10/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.8) (2025.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8->lm_eval==0.4.8) (3.0.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->lm_eval==0.4.8) (2025.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate>=0.26.0->lm_eval==0.4.8) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate>=0.26.0->lm_eval==0.4.8) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate>=0.26.0->lm_eval==0.4.8) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate>=0.26.0->lm_eval==0.4.8) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3.0.0,>=1.17->accelerate>=0.26.0->lm_eval==0.4.8) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\nDownloading pytablewriter-1.2.1-py3-none-any.whl (91 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\nDownloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading DataProperty-1.1.0-py3-none-any.whl (27 kB)\nDownloading mbstrdecoder-1.1.4-py3-none-any.whl (7.9 kB)\nDownloading pathvalidate-3.2.3-py3-none-any.whl (24 kB)\nDownloading tabledata-1.3.4-py3-none-any.whl (11 kB)\nDownloading tcolorpy-0.1.7-py3-none-any.whl (8.1 kB)\nDownloading typepy-1.3.4-py3-none-any.whl (31 kB)\nDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\nBuilding wheels for collected packages: lm_eval, rouge-score, sqlitedict, word2number\n  Building editable for lm_eval (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for lm_eval: filename=lm_eval-0.4.8-0.editable-py3-none-any.whl size=24668 sha256=b9a6611355987be9060a28b7ccb3810446958290ea49becbeef899f0cdda8e52\n  Stored in directory: /tmp/pip-ephem-wheel-cache-oyy9_ayh/wheels/1b/1a/1b/44c80ddb18c9d7d3ce79a8d6d4561bddaddcbffb4cdfbf3259\n  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=65b9e59052f81a3b91e38edee4e6c9529346ce62f8ba2031e000ee27cda9a1bd\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16864 sha256=aaf92795f641992256915dfafa2d29eb47a0a362ad2275630a902c5b7490eb34\n  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5568 sha256=7f1c5bfa935e34aa19303dfc1216ae33777c8913b2afc1059364baaaa8168a04\n  Stored in directory: /root/.cache/pip/wheels/84/ff/26/d3cfbd971e96c5aa3737ecfced81628830d7359b55fbb8ca3b\nSuccessfully built lm_eval rouge-score sqlitedict word2number\nInstalling collected packages: word2number, sqlitedict, zstandard, tqdm-multiprocess, tcolorpy, portalocker, pathvalidate, mbstrdecoder, jsonlines, typepy, DataProperty, tabledata, pytablewriter, sacrebleu, rouge-score, evaluate, lm_eval\nSuccessfully installed DataProperty-1.1.0 evaluate-0.4.3 jsonlines-4.0.0 lm_eval-0.4.8 mbstrdecoder-1.1.4 pathvalidate-3.2.3 portalocker-3.1.1 pytablewriter-1.2.1 rouge-score-0.1.2 sacrebleu-2.5.1 sqlitedict-2.1.0 tabledata-1.3.4 tcolorpy-0.1.7 tqdm-multiprocess-0.0.11 typepy-1.3.4 word2number-1.1 zstandard-0.23.0\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"#for finetunned model command  for bench marks","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !python -m lm_eval \\\n#   --model hf \\\n#   --model_args pretrained=sparky353454/gemma-2b-gsm8k,use_auth_token=True \\\n#   --tasks gsm8k \\\n#   --device cuda \\\n#   --output_path results.json\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for base model benchmarks ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !python -m lm_eval \\\n#   --model hf \\\n#   --model_args pretrained=google/gemma-2-2b-it,use_auth_token=True \\\n#   --tasks gsm8k \\\n#   --device cuda \\\n#   --output_path results_base.json\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-06T13:10:31.512Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel, PeftConfig\n\n# Your model names\nlora_model_name = \"sparky353454/gemma-2b-gsm8k\"   # Fine-tuned LoRA model\nbase_model_name = \"google/gemma-2-2b-it\"              # Base model\n\n# Load PEFT config from the LoRA model\npeft_config = PeftConfig.from_pretrained(lora_model_name)\n\n# Load base model\nbase_model = AutoModelForCausalLM.from_pretrained(\n    base_model_name,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\"\n)\n\n# Load the LoRA adapter weights\nmodel = PeftModel.from_pretrained(base_model, lora_model_name)\n\n# Merge LoRA into base weights\nmodel = model.merge_and_unload()\n\n# Save the merged model locally\nmerged_model_dir = \"gemma-2b-gsm8k-merged\"\nmodel.save_pretrained(merged_model_dir)\n\n# Save tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\ntokenizer.save_pretrained(merged_model_dir)\n\nprint(f\"✅ Merged model saved to: {merged_model_dir}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T16:12:04.490584Z","iopub.execute_input":"2025-04-06T16:12:04.490944Z","iopub.status.idle":"2025-04-06T16:12:50.810192Z","shell.execute_reply.started":"2025-04-06T16:12:04.490895Z","shell.execute_reply":"2025-04-06T16:12:50.809303Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/838 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55e5f5d36c4f426abedca9bb6e8c5e2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69dce7842dfb4b2680db1fece4f63c52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc48159d778844a7b4c5c5679962d606"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19090d7fcdcd4d8db773161e03328e48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ccba914ef584b1a809f9003181661b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2be03cae8ff64d3cbc4f85765291718e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"690258cce9e4441d8b31ef1b5de2dc84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4504f22a316457e814220f067fed94a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19b1ff4fe196444dabe51232b115f238"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9219fdbae4b3419a8df9c4e593fed965"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"290cae5939d649bab221252cf3a6e096"}},"metadata":{}},{"name":"stdout","text":"✅ Merged model saved to: gemma-2b-gsm8k-merged\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Path to the merged model directory\nmodel_path = \"/kaggle/working/gemma-2b-gsm8k-merged\"  # Make sure this folder contains config.json, pytorch_model.bin, tokenizer files, etc.\n\n# Load the merged model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# Push to Hugging Face Hub\nmodel.push_to_hub(\"sparky353454/gemma-2-2b-it-gsm8k-merged\")\ntokenizer.push_to_hub(\"sparky353454/gemma-2-2b-it-gsm8k-merged\")\n\nprint(\"✅ Merged model and tokenizer pushed to Hugging Face Hub successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T16:22:01.643093Z","iopub.execute_input":"2025-04-06T16:22:01.643437Z","iopub.status.idle":"2025-04-06T16:25:16.769517Z","shell.execute_reply.started":"2025-04-06T16:22:01.643411Z","shell.execute_reply":"2025-04-06T16:25:16.768577Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"607a4a028c6b4309ae02cbc3040840f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/481M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6c6c98f1bbb4508ab53cb55d3a2194c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f0d36b28a9e4258b87d7b127a51d141"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45e3037aa8194d738fe83d8b83465554"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29c9f41da186418ab39a920a119d60c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09795258a74b446a9382f8ef06cb1561"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ff29e05dd4842f3a316067a751443aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/34.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5882b9059ae416986fd721ee64bcbe3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e36b784831364412a8b243b9d2cc31c7"}},"metadata":{}},{"name":"stdout","text":"✅ Merged model and tokenizer pushed to Hugging Face Hub successfully!\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import torch\nimport gradio as gr\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Load the fine-tuned model from Hugging Face\nmodel_name = \"sparky353454/gemma-2-2b-it-gsm8k-merged\"  \ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n\n# Function to generate response\ndef generate_answer(question):\n    prompt = f\"Question: {question}\\nAnswer:\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n    with torch.no_grad():\n        output = model.generate(**inputs, max_new_tokens=200)\n    \n    answer = tokenizer.decode(output[0], skip_special_tokens=True)\n    return answer.split(\"Answer:\")[-1].strip()\n\n# Create Gradio UI\niface = gr.Interface(\n    fn=generate_answer,\n    inputs=gr.Textbox(label=\"Enter Your Math Question\"),\n    outputs=gr.Textbox(label=\"Model's Answer\"),\n    title=\"Gemma 2B - GSM8K Math Solver\",\n    description=\"Enter a mathematical reasoning question, and the fine-tuned Gemma 2B model will generate the answer.\"\n)\n\n# Launch the app\niface.launch()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T16:30:58.153590Z","iopub.execute_input":"2025-04-06T16:30:58.153917Z","iopub.status.idle":"2025-04-06T16:32:12.557806Z","shell.execute_reply.started":"2025-04-06T16:30:58.153888Z","shell.execute_reply":"2025-04-06T16:32:12.557158Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8427429d65a94b7da073951e06fae2b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"419a905e9ace45498a1d7940ef2cccaf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/34.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09aba1e614914d5aba817f8443769e8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a0485c1576f4d51a6b352164e3ae604"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/837 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84731486d3454194b123dc6a1eaf9774"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a63f8b76c8d44e58957278b6adc69099"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f8bb3f2d4454befb430a19fe4ee6e57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d0b3a2265e7451698cb4a0420782e8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee299d5250fb48e69a28ffb6610b94b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/481M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8b812f4aefd421c9d854355686ebbe9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5152b8b2c3fe45889c6ae5e187c78a90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fca4f652e365440aa0a7d541f3c85e53"}},"metadata":{}},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\nKaggle notebooks require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://bd59d572e775b9bf8f.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://bd59d572e775b9bf8f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value |   |Stderr|\n|-----|------:|----------------|-----:|-----------|---|-----:|---|-----:|\n|gsm8k|      3|flexible-extract|     5|exact_match|↑  |0.4556|±  |0.0137|\n|     |       |strict-match    |     5|exact_match|↑  |0.4496|±  |0.0137|\n\n\n#basemodel benchmarks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T16:46:04.551424Z","iopub.execute_input":"2025-04-06T16:46:04.551772Z","iopub.status.idle":"2025-04-06T16:46:04.558588Z","shell.execute_reply.started":"2025-04-06T16:46:04.551742Z","shell.execute_reply":"2025-04-06T16:46:04.557515Z"}},"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-24723f08d6c5>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    |gsm8k|      3|flexible-extract|     5|exact_match|↑  |0.4556|±  |0.0137|\u001b[0m\n\u001b[0m                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '↑' (U+2191)\n"],"ename":"SyntaxError","evalue":"invalid character '↑' (U+2191) (<ipython-input-8-24723f08d6c5>, line 3)","output_type":"error"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}