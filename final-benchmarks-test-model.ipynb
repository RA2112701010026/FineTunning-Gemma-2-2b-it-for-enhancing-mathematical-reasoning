{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch gradio transformers\n!pip install bitsandbytes \n!pip install peft\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:15:11.702910Z","iopub.execute_input":"2025-05-12T07:15:11.703216Z","iopub.status.idle":"2025-05-12T07:15:31.589721Z","shell.execute_reply.started":"2025-05-12T07:15:11.703186Z","shell.execute_reply":"2025-05-12T07:15:31.588256Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nCollecting gradio\n  Downloading gradio-5.29.0-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (22.1.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\nCollecting fastapi<1.0,>=0.115.2 (from gradio)\n  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\nCollecting ffmpy (from gradio)\n  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\nCollecting gradio-client==1.10.0 (from gradio)\n  Downloading gradio_client-1.10.0-py3-none-any.whl.metadata (7.1 kB)\nCollecting groovy~=0.1 (from gradio)\n  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\nRequirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.1)\nRequirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.29.0)\nRequirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.0.2)\nRequirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\nRequirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.12)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\nRequirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.3)\nRequirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\nRequirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.11.0a2)\nRequirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\nCollecting python-multipart>=0.0.18 (from gradio)\n  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\nCollecting ruff>=0.9.3 (from gradio)\n  Downloading ruff-0.11.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\nCollecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\nCollecting semantic-version~=2.0 (from gradio)\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\nCollecting starlette<1.0,>=0.40.0 (from gradio)\n  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\nCollecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.15.1)\nCollecting uvicorn>=0.14.0 (from gradio)\n  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.10.0->gradio) (14.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.29.0)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0,>=1.0->gradio) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.0->gradio) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\nDownloading gradio-5.29.0-py3-none-any.whl (54.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading gradio_client-1.10.0-py3-none-any.whl (322 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.9/322.9 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\nDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\nDownloading ruff-0.11.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m104.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\nDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\nDownloading starlette-0.46.2-py3-none-any.whl (72 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\nDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\nInstalling collected packages: uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, starlette, safehttpx, gradio-client, fastapi, gradio\nSuccessfully installed fastapi-0.115.12 ffmpy-0.5.0 gradio-5.29.0 gradio-client-1.10.0 groovy-0.1.2 python-multipart-0.0.20 ruff-0.11.9 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.2\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.45.5\nRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.1+cu121)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.47.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.67.1)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.2.1)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.25.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.29.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (3.17.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (2024.12.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.21.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->peft) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->peft) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2025.1.31)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->peft) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport torch\nimport wandb\n\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    logging,\n)\n\nfrom peft import (\n    LoraConfig,\n    get_peft_model,\n    prepare_model_for_kbit_training,\n)\n\nfrom datasets import load_dataset\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\n# Optional: for training with TRL's SFTTrainer\n# from trl import SFTTrainer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:26:10.949317Z","iopub.execute_input":"2025-05-12T07:26:10.949667Z","iopub.status.idle":"2025-05-12T07:26:33.982386Z","shell.execute_reply.started":"2025-05-12T07:26:10.949644Z","shell.execute_reply":"2025-05-12T07:26:33.981574Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"user_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nwb_token = user_secrets.get_secret(\"wandb\")\n\nlogin(token=hf_token)\nwandb.login(key=wb_token)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:26:35.534768Z","iopub.execute_input":"2025-05-12T07:26:35.535361Z","iopub.status.idle":"2025-05-12T07:26:41.916040Z","shell.execute_reply.started":"2025-05-12T07:26:35.535334Z","shell.execute_reply":"2025-05-12T07:26:41.915276Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33muu0712\u001b[0m (\u001b[33muu0712-engineering-student-council\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\nquant_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=\"bfloat16\"\n)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:26:41.917193Z","iopub.execute_input":"2025-05-12T07:26:41.917852Z","iopub.status.idle":"2025-05-12T07:26:41.923346Z","shell.execute_reply.started":"2025-05-12T07:26:41.917821Z","shell.execute_reply":"2025-05-12T07:26:41.922581Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import torch\nimport gradio as gr\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Load the fine-tuned model from Hugging Face\nmodel_name = \"sparky353454/last_latest_gemma_2_2b_it\"  \ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n\n# Function to generate response\ndef generate_answer(question):\n    prompt = f\"Question: {question}\\nAnswer:\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n    with torch.no_grad():\n        output = model.generate(**inputs, max_new_tokens=200)\n    \n    answer = tokenizer.decode(output[0], skip_special_tokens=True)\n    return answer.split(\"Answer:\")[-1].strip()\n\n# Create Gradio UI\niface = gr.Interface(\n    fn=generate_answer,\n    inputs=gr.Textbox(label=\"Enter Your Math Question\"),\n    outputs=gr.Textbox(label=\"Model's Answer\"),\n    title=\"Gemma 2B - GSM8K Math Solver\",\n    description=\"Enter a mathematical reasoning question, and the fine-tuned Gemma 2B model will generate the answer.\"\n)\n\n# Launch the app\niface.launch()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:26:53.011530Z","iopub.execute_input":"2025-05-12T07:26:53.011819Z","iopub.status.idle":"2025-05-12T07:27:31.722854Z","shell.execute_reply.started":"2025-05-12T07:26:53.011797Z","shell.execute_reply":"2025-05-12T07:27:31.722198Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/47.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bc94039a9074f0282ac7f1265b08de4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/34.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01182a76f0874e229a62abb0ed8ac936"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4108c4dd21f54ce48153de55b0ac9910"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/851 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84e3a7f87ea34d17af63d0426f7b79c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/838 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3aeacf75e8e2479696ce2f36592d2a9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b84533270084883b3e06d3003cf1177"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c76e5b6831ee4cf9bf645c8909aca41f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc63868ef3ff4c8d8cf1911147edea63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26087afc2b12437d8d56065ad1032d79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76bb426ec91a4e039b20159b63024fb5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86bee2cf1f69400395618edf687cb8f9"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/peft/config.py:162: UserWarning: Unexpected keyword arguments ['alpha_pattern', 'bias', 'corda_config', 'eva_config', 'exclude_modules', 'fan_in_fan_out', 'init_lora_weights', 'layer_replication', 'layers_pattern', 'layers_to_transform', 'loftq_config', 'lora_alpha', 'lora_bias', 'lora_dropout', 'megatron_config', 'megatron_core', 'modules_to_save', 'r', 'rank_pattern', 'target_modules', 'trainable_token_indices', 'use_dora', 'use_rslora'] for class PeftConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/41.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b9fd128817341d7911c670aba1a34a6"}},"metadata":{}},{"name":"stderr","text":"Loading adapter weights from sparky353454/last_latest_gemma_2_2b_it led to unexpected keys not found in the model: model.layers.0.self_attn.k_proj.lora_A.default.weight, model.layers.0.self_attn.k_proj.lora_B.default.weight, model.layers.0.self_attn.o_proj.lora_A.default.weight, model.layers.0.self_attn.o_proj.lora_B.default.weight, model.layers.0.mlp.gate_proj.lora_A.default.weight, model.layers.0.mlp.gate_proj.lora_B.default.weight, model.layers.0.mlp.up_proj.lora_A.default.weight, model.layers.0.mlp.up_proj.lora_B.default.weight, model.layers.0.mlp.down_proj.lora_A.default.weight, model.layers.0.mlp.down_proj.lora_B.default.weight, model.layers.1.self_attn.k_proj.lora_A.default.weight, model.layers.1.self_attn.k_proj.lora_B.default.weight, model.layers.1.self_attn.o_proj.lora_A.default.weight, model.layers.1.self_attn.o_proj.lora_B.default.weight, model.layers.1.mlp.gate_proj.lora_A.default.weight, model.layers.1.mlp.gate_proj.lora_B.default.weight, model.layers.1.mlp.up_proj.lora_A.default.weight, model.layers.1.mlp.up_proj.lora_B.default.weight, model.layers.1.mlp.down_proj.lora_A.default.weight, model.layers.1.mlp.down_proj.lora_B.default.weight, model.layers.2.self_attn.k_proj.lora_A.default.weight, model.layers.2.self_attn.k_proj.lora_B.default.weight, model.layers.2.self_attn.o_proj.lora_A.default.weight, model.layers.2.self_attn.o_proj.lora_B.default.weight, model.layers.2.mlp.gate_proj.lora_A.default.weight, model.layers.2.mlp.gate_proj.lora_B.default.weight, model.layers.2.mlp.up_proj.lora_A.default.weight, model.layers.2.mlp.up_proj.lora_B.default.weight, model.layers.2.mlp.down_proj.lora_A.default.weight, model.layers.2.mlp.down_proj.lora_B.default.weight, model.layers.3.self_attn.k_proj.lora_A.default.weight, model.layers.3.self_attn.k_proj.lora_B.default.weight, model.layers.3.self_attn.o_proj.lora_A.default.weight, model.layers.3.self_attn.o_proj.lora_B.default.weight, model.layers.3.mlp.gate_proj.lora_A.default.weight, model.layers.3.mlp.gate_proj.lora_B.default.weight, model.layers.3.mlp.up_proj.lora_A.default.weight, model.layers.3.mlp.up_proj.lora_B.default.weight, model.layers.3.mlp.down_proj.lora_A.default.weight, model.layers.3.mlp.down_proj.lora_B.default.weight, model.layers.4.self_attn.k_proj.lora_A.default.weight, model.layers.4.self_attn.k_proj.lora_B.default.weight, model.layers.4.self_attn.o_proj.lora_A.default.weight, model.layers.4.self_attn.o_proj.lora_B.default.weight, model.layers.4.mlp.gate_proj.lora_A.default.weight, model.layers.4.mlp.gate_proj.lora_B.default.weight, model.layers.4.mlp.up_proj.lora_A.default.weight, model.layers.4.mlp.up_proj.lora_B.default.weight, model.layers.4.mlp.down_proj.lora_A.default.weight, model.layers.4.mlp.down_proj.lora_B.default.weight, model.layers.5.self_attn.k_proj.lora_A.default.weight, model.layers.5.self_attn.k_proj.lora_B.default.weight, model.layers.5.self_attn.o_proj.lora_A.default.weight, model.layers.5.self_attn.o_proj.lora_B.default.weight, model.layers.5.mlp.gate_proj.lora_A.default.weight, model.layers.5.mlp.gate_proj.lora_B.default.weight, model.layers.5.mlp.up_proj.lora_A.default.weight, model.layers.5.mlp.up_proj.lora_B.default.weight, model.layers.5.mlp.down_proj.lora_A.default.weight, model.layers.5.mlp.down_proj.lora_B.default.weight, model.layers.6.self_attn.k_proj.lora_A.default.weight, model.layers.6.self_attn.k_proj.lora_B.default.weight, model.layers.6.self_attn.o_proj.lora_A.default.weight, model.layers.6.self_attn.o_proj.lora_B.default.weight, model.layers.6.mlp.gate_proj.lora_A.default.weight, model.layers.6.mlp.gate_proj.lora_B.default.weight, model.layers.6.mlp.up_proj.lora_A.default.weight, model.layers.6.mlp.up_proj.lora_B.default.weight, model.layers.6.mlp.down_proj.lora_A.default.weight, model.layers.6.mlp.down_proj.lora_B.default.weight, model.layers.7.self_attn.k_proj.lora_A.default.weight, model.layers.7.self_attn.k_proj.lora_B.default.weight, model.layers.7.self_attn.o_proj.lora_A.default.weight, model.layers.7.self_attn.o_proj.lora_B.default.weight, model.layers.7.mlp.gate_proj.lora_A.default.weight, model.layers.7.mlp.gate_proj.lora_B.default.weight, model.layers.7.mlp.up_proj.lora_A.default.weight, model.layers.7.mlp.up_proj.lora_B.default.weight, model.layers.7.mlp.down_proj.lora_A.default.weight, model.layers.7.mlp.down_proj.lora_B.default.weight, model.layers.8.self_attn.k_proj.lora_A.default.weight, model.layers.8.self_attn.k_proj.lora_B.default.weight, model.layers.8.self_attn.o_proj.lora_A.default.weight, model.layers.8.self_attn.o_proj.lora_B.default.weight, model.layers.8.mlp.gate_proj.lora_A.default.weight, model.layers.8.mlp.gate_proj.lora_B.default.weight, model.layers.8.mlp.up_proj.lora_A.default.weight, model.layers.8.mlp.up_proj.lora_B.default.weight, model.layers.8.mlp.down_proj.lora_A.default.weight, model.layers.8.mlp.down_proj.lora_B.default.weight, model.layers.9.self_attn.k_proj.lora_A.default.weight, model.layers.9.self_attn.k_proj.lora_B.default.weight, model.layers.9.self_attn.o_proj.lora_A.default.weight, model.layers.9.self_attn.o_proj.lora_B.default.weight, model.layers.9.mlp.gate_proj.lora_A.default.weight, model.layers.9.mlp.gate_proj.lora_B.default.weight, model.layers.9.mlp.up_proj.lora_A.default.weight, model.layers.9.mlp.up_proj.lora_B.default.weight, model.layers.9.mlp.down_proj.lora_A.default.weight, model.layers.9.mlp.down_proj.lora_B.default.weight, model.layers.10.self_attn.k_proj.lora_A.default.weight, model.layers.10.self_attn.k_proj.lora_B.default.weight, model.layers.10.self_attn.o_proj.lora_A.default.weight, model.layers.10.self_attn.o_proj.lora_B.default.weight, model.layers.10.mlp.gate_proj.lora_A.default.weight, model.layers.10.mlp.gate_proj.lora_B.default.weight, model.layers.10.mlp.up_proj.lora_A.default.weight, model.layers.10.mlp.up_proj.lora_B.default.weight, model.layers.10.mlp.down_proj.lora_A.default.weight, model.layers.10.mlp.down_proj.lora_B.default.weight, model.layers.11.self_attn.k_proj.lora_A.default.weight, model.layers.11.self_attn.k_proj.lora_B.default.weight, model.layers.11.self_attn.o_proj.lora_A.default.weight, model.layers.11.self_attn.o_proj.lora_B.default.weight, model.layers.11.mlp.gate_proj.lora_A.default.weight, model.layers.11.mlp.gate_proj.lora_B.default.weight, model.layers.11.mlp.up_proj.lora_A.default.weight, model.layers.11.mlp.up_proj.lora_B.default.weight, model.layers.11.mlp.down_proj.lora_A.default.weight, model.layers.11.mlp.down_proj.lora_B.default.weight, model.layers.12.self_attn.k_proj.lora_A.default.weight, model.layers.12.self_attn.k_proj.lora_B.default.weight, model.layers.12.self_attn.o_proj.lora_A.default.weight, model.layers.12.self_attn.o_proj.lora_B.default.weight, model.layers.12.mlp.gate_proj.lora_A.default.weight, model.layers.12.mlp.gate_proj.lora_B.default.weight, model.layers.12.mlp.up_proj.lora_A.default.weight, model.layers.12.mlp.up_proj.lora_B.default.weight, model.layers.12.mlp.down_proj.lora_A.default.weight, model.layers.12.mlp.down_proj.lora_B.default.weight, model.layers.13.self_attn.k_proj.lora_A.default.weight, model.layers.13.self_attn.k_proj.lora_B.default.weight, model.layers.13.self_attn.o_proj.lora_A.default.weight, model.layers.13.self_attn.o_proj.lora_B.default.weight, model.layers.13.mlp.gate_proj.lora_A.default.weight, model.layers.13.mlp.gate_proj.lora_B.default.weight, model.layers.13.mlp.up_proj.lora_A.default.weight, model.layers.13.mlp.up_proj.lora_B.default.weight, model.layers.13.mlp.down_proj.lora_A.default.weight, model.layers.13.mlp.down_proj.lora_B.default.weight, model.layers.14.self_attn.k_proj.lora_A.default.weight, model.layers.14.self_attn.k_proj.lora_B.default.weight, model.layers.14.self_attn.o_proj.lora_A.default.weight, model.layers.14.self_attn.o_proj.lora_B.default.weight, model.layers.14.mlp.gate_proj.lora_A.default.weight, model.layers.14.mlp.gate_proj.lora_B.default.weight, model.layers.14.mlp.up_proj.lora_A.default.weight, model.layers.14.mlp.up_proj.lora_B.default.weight, model.layers.14.mlp.down_proj.lora_A.default.weight, model.layers.14.mlp.down_proj.lora_B.default.weight, model.layers.15.self_attn.k_proj.lora_A.default.weight, model.layers.15.self_attn.k_proj.lora_B.default.weight, model.layers.15.self_attn.o_proj.lora_A.default.weight, model.layers.15.self_attn.o_proj.lora_B.default.weight, model.layers.15.mlp.gate_proj.lora_A.default.weight, model.layers.15.mlp.gate_proj.lora_B.default.weight, model.layers.15.mlp.up_proj.lora_A.default.weight, model.layers.15.mlp.up_proj.lora_B.default.weight, model.layers.15.mlp.down_proj.lora_A.default.weight, model.layers.15.mlp.down_proj.lora_B.default.weight, model.layers.16.self_attn.k_proj.lora_A.default.weight, model.layers.16.self_attn.k_proj.lora_B.default.weight, model.layers.16.self_attn.o_proj.lora_A.default.weight, model.layers.16.self_attn.o_proj.lora_B.default.weight, model.layers.16.mlp.gate_proj.lora_A.default.weight, model.layers.16.mlp.gate_proj.lora_B.default.weight, model.layers.16.mlp.up_proj.lora_A.default.weight, model.layers.16.mlp.up_proj.lora_B.default.weight, model.layers.16.mlp.down_proj.lora_A.default.weight, model.layers.16.mlp.down_proj.lora_B.default.weight, model.layers.17.self_attn.k_proj.lora_A.default.weight, model.layers.17.self_attn.k_proj.lora_B.default.weight, model.layers.17.self_attn.o_proj.lora_A.default.weight, model.layers.17.self_attn.o_proj.lora_B.default.weight, model.layers.17.mlp.gate_proj.lora_A.default.weight, model.layers.17.mlp.gate_proj.lora_B.default.weight, model.layers.17.mlp.up_proj.lora_A.default.weight, model.layers.17.mlp.up_proj.lora_B.default.weight, model.layers.17.mlp.down_proj.lora_A.default.weight, model.layers.17.mlp.down_proj.lora_B.default.weight, model.layers.18.self_attn.k_proj.lora_A.default.weight, model.layers.18.self_attn.k_proj.lora_B.default.weight, model.layers.18.self_attn.o_proj.lora_A.default.weight, model.layers.18.self_attn.o_proj.lora_B.default.weight, model.layers.18.mlp.gate_proj.lora_A.default.weight, model.layers.18.mlp.gate_proj.lora_B.default.weight, model.layers.18.mlp.up_proj.lora_A.default.weight, model.layers.18.mlp.up_proj.lora_B.default.weight, model.layers.18.mlp.down_proj.lora_A.default.weight, model.layers.18.mlp.down_proj.lora_B.default.weight, model.layers.19.self_attn.k_proj.lora_A.default.weight, model.layers.19.self_attn.k_proj.lora_B.default.weight, model.layers.19.self_attn.o_proj.lora_A.default.weight, model.layers.19.self_attn.o_proj.lora_B.default.weight, model.layers.19.mlp.gate_proj.lora_A.default.weight, model.layers.19.mlp.gate_proj.lora_B.default.weight, model.layers.19.mlp.up_proj.lora_A.default.weight, model.layers.19.mlp.up_proj.lora_B.default.weight, model.layers.19.mlp.down_proj.lora_A.default.weight, model.layers.19.mlp.down_proj.lora_B.default.weight, model.layers.20.self_attn.k_proj.lora_A.default.weight, model.layers.20.self_attn.k_proj.lora_B.default.weight, model.layers.20.self_attn.o_proj.lora_A.default.weight, model.layers.20.self_attn.o_proj.lora_B.default.weight, model.layers.20.mlp.gate_proj.lora_A.default.weight, model.layers.20.mlp.gate_proj.lora_B.default.weight, model.layers.20.mlp.up_proj.lora_A.default.weight, model.layers.20.mlp.up_proj.lora_B.default.weight, model.layers.20.mlp.down_proj.lora_A.default.weight, model.layers.20.mlp.down_proj.lora_B.default.weight, model.layers.21.self_attn.k_proj.lora_A.default.weight, model.layers.21.self_attn.k_proj.lora_B.default.weight, model.layers.21.self_attn.o_proj.lora_A.default.weight, model.layers.21.self_attn.o_proj.lora_B.default.weight, model.layers.21.mlp.gate_proj.lora_A.default.weight, model.layers.21.mlp.gate_proj.lora_B.default.weight, model.layers.21.mlp.up_proj.lora_A.default.weight, model.layers.21.mlp.up_proj.lora_B.default.weight, model.layers.21.mlp.down_proj.lora_A.default.weight, model.layers.21.mlp.down_proj.lora_B.default.weight, model.layers.22.self_attn.k_proj.lora_A.default.weight, model.layers.22.self_attn.k_proj.lora_B.default.weight, model.layers.22.self_attn.o_proj.lora_A.default.weight, model.layers.22.self_attn.o_proj.lora_B.default.weight, model.layers.22.mlp.gate_proj.lora_A.default.weight, model.layers.22.mlp.gate_proj.lora_B.default.weight, model.layers.22.mlp.up_proj.lora_A.default.weight, model.layers.22.mlp.up_proj.lora_B.default.weight, model.layers.22.mlp.down_proj.lora_A.default.weight, model.layers.22.mlp.down_proj.lora_B.default.weight, model.layers.23.self_attn.k_proj.lora_A.default.weight, model.layers.23.self_attn.k_proj.lora_B.default.weight, model.layers.23.self_attn.o_proj.lora_A.default.weight, model.layers.23.self_attn.o_proj.lora_B.default.weight, model.layers.23.mlp.gate_proj.lora_A.default.weight, model.layers.23.mlp.gate_proj.lora_B.default.weight, model.layers.23.mlp.up_proj.lora_A.default.weight, model.layers.23.mlp.up_proj.lora_B.default.weight, model.layers.23.mlp.down_proj.lora_A.default.weight, model.layers.23.mlp.down_proj.lora_B.default.weight, model.layers.24.self_attn.k_proj.lora_A.default.weight, model.layers.24.self_attn.k_proj.lora_B.default.weight, model.layers.24.self_attn.o_proj.lora_A.default.weight, model.layers.24.self_attn.o_proj.lora_B.default.weight, model.layers.24.mlp.gate_proj.lora_A.default.weight, model.layers.24.mlp.gate_proj.lora_B.default.weight, model.layers.24.mlp.up_proj.lora_A.default.weight, model.layers.24.mlp.up_proj.lora_B.default.weight, model.layers.24.mlp.down_proj.lora_A.default.weight, model.layers.24.mlp.down_proj.lora_B.default.weight, model.layers.25.self_attn.k_proj.lora_A.default.weight, model.layers.25.self_attn.k_proj.lora_B.default.weight, model.layers.25.self_attn.o_proj.lora_A.default.weight, model.layers.25.self_attn.o_proj.lora_B.default.weight, model.layers.25.mlp.gate_proj.lora_A.default.weight, model.layers.25.mlp.gate_proj.lora_B.default.weight, model.layers.25.mlp.up_proj.lora_A.default.weight, model.layers.25.mlp.up_proj.lora_B.default.weight, model.layers.25.mlp.down_proj.lora_A.default.weight, model.layers.25.mlp.down_proj.lora_B.default.weight. \n","output_type":"stream"},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\nIt looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://512363396c8527c719.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://512363396c8527c719.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}},{"name":"stderr","text":"The 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Jack is 4 years older than Jill. In 6 years, Jack will be twice as old as Jill. How old are they now?\n# A car travels 150 km on 10 liters of fuel. How far can it travel on 25 liters?\n\n# The sum of three consecutive odd numbers is 75. What are the numbers?\n\n# If 5 workers complete a task in 12 days, how many days would it take for 10 workers to do the same task?","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#basemodel test using grado ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport gradio as gr\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Load the fine-tuned model\nmodel_name = \"google/gemma-2-2b-it\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n\n# Function to generate response\ndef generate_answer(question):\n    prompt = f\"Question: {question}\\nAnswer:\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n    \n    with torch.no_grad():\n        output = model.generate(\n            **inputs,\n            max_new_tokens=512,\n            eos_token_id=tokenizer.eos_token_id,\n            pad_token_id=tokenizer.pad_token_id,\n        )\n    \n    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n    answer = decoded_output[len(prompt):].strip()\n    return answer\n\n# Gradio UI\niface = gr.Interface(\n    fn=generate_answer,\n    inputs=gr.Textbox(label=\"Enter Your Math Question\"),\n    outputs=gr.Textbox(label=\"Model's Answer\"),\n    title=\"Gemma 2B - GSM8K Math Solver\",\n    description=\"Enter a mathematical reasoning question, and the fine-tuned Gemma 2B model will generate the answer.\"\n)\n\niface.launch()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#benchmarking ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import getpass\n\n# Prompt for GitHub token\ntoken = getpass.getpass(\"Enter your GitHub token (starts with 'ghp_' or 'github_pat_...'): \")\n\n# Use EleutherAI repo instead of OpenAI\nrepo_url = f\"https://{token}:x-oauth-basic@github.com/EleutherAI/lm-evaluation-harness.git\"\n\n# Clone it\n!git clone {repo_url}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T06:43:38.682970Z","iopub.execute_input":"2025-04-24T06:43:38.683318Z","iopub.status.idle":"2025-04-24T06:43:47.553310Z","shell.execute_reply.started":"2025-04-24T06:43:38.683297Z","shell.execute_reply":"2025-04-24T06:43:47.552398Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Enter your GitHub token (starts with 'ghp_' or 'github_pat_...'):  ········\n"},{"name":"stdout","text":"Cloning into 'lm-evaluation-harness'...\nremote: Enumerating objects: 49683, done.\u001b[K\nremote: Counting objects: 100% (99/99), done.\u001b[K\nremote: Compressing objects: 100% (91/91), done.\u001b[K\nremote: Total 49683 (delta 74), reused 8 (delta 8), pack-reused 49584 (from 3)\u001b[K\nReceiving objects: 100% (49683/49683), 29.69 MiB | 21.16 MiB/s, done.\nResolving deltas: 100% (34367/34367), done.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"#installation","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd lm-evaluation-harness\n!pip install -e .\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T06:43:51.903910Z","iopub.execute_input":"2025-04-24T06:43:51.904238Z","iopub.status.idle":"2025-04-24T06:44:13.805241Z","shell.execute_reply.started":"2025-04-24T06:43:51.904215Z","shell.execute_reply":"2025-04-24T06:44:13.804137Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/lm-evaluation-harness\nObtaining file:///kaggle/working/lm-evaluation-harness\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.8) (1.2.1)\nCollecting evaluate (from lm_eval==0.4.8)\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.8) (3.3.1)\nCollecting jsonlines (from lm_eval==0.4.8)\n  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.8) (2.10.2)\nRequirement already satisfied: peft>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.8) (0.14.0)\nRequirement already satisfied: pybind11>=2.6.2 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.8) (2.13.6)\nCollecting pytablewriter (from lm_eval==0.4.8)\n  Downloading pytablewriter-1.2.1-py3-none-any.whl.metadata (38 kB)\nCollecting rouge-score>=0.0.4 (from lm_eval==0.4.8)\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting sacrebleu>=1.5.0 (from lm_eval==0.4.8)\n  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.8) (1.2.2)\nCollecting sqlitedict (from lm_eval==0.4.8)\n  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: torch>=1.8 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.8) (2.5.1+cu121)\nCollecting tqdm-multiprocess (from lm_eval==0.4.8)\n  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl.metadata (5.7 kB)\nRequirement already satisfied: transformers>=4.1 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.8) (4.47.0)\nCollecting zstandard (from lm_eval==0.4.8)\n  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\nRequirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.8) (0.3.8)\nCollecting word2number (from lm_eval==0.4.8)\n  Downloading word2number-1.1.zip (9.7 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: more_itertools in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.8) (10.5.0)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.8) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.8) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.8) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.8) (6.0.2)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.8) (0.29.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.8) (0.4.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm_eval==0.4.8) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm_eval==0.4.8) (19.0.1)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm_eval==0.4.8) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm_eval==0.4.8) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm_eval==0.4.8) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm_eval==0.4.8) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm_eval==0.4.8) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.8) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm_eval==0.4.8) (3.11.12)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm_eval==0.4.8) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm_eval==0.4.8) (3.2.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm_eval==0.4.8) (1.17.0)\nCollecting portalocker (from sacrebleu>=1.5.0->lm_eval==0.4.8)\n  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.8) (2024.11.6)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.8) (0.9.0)\nRequirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.8) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.8) (5.3.0)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.4.8) (1.13.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.4.8) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.4.8) (3.5.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==0.4.8) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==0.4.8) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==0.4.8) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==0.4.8) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8->lm_eval==0.4.8) (1.3.0)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.1->lm_eval==0.4.8) (0.21.0)\nRequirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines->lm_eval==0.4.8) (25.1.0)\nRequirement already satisfied: setuptools>=38.3.0 in /usr/local/lib/python3.10/dist-packages (from pytablewriter->lm_eval==0.4.8) (75.1.0)\nCollecting DataProperty<2,>=1.1.0 (from pytablewriter->lm_eval==0.4.8)\n  Downloading DataProperty-1.1.0-py3-none-any.whl.metadata (11 kB)\nCollecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm_eval==0.4.8)\n  Downloading mbstrdecoder-1.1.4-py3-none-any.whl.metadata (4.3 kB)\nCollecting pathvalidate<4,>=2.3.0 (from pytablewriter->lm_eval==0.4.8)\n  Downloading pathvalidate-3.2.3-py3-none-any.whl.metadata (12 kB)\nCollecting tabledata<2,>=1.3.1 (from pytablewriter->lm_eval==0.4.8)\n  Downloading tabledata-1.3.4-py3-none-any.whl.metadata (3.7 kB)\nCollecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm_eval==0.4.8)\n  Downloading tcolorpy-0.1.7-py3-none-any.whl.metadata (6.3 kB)\nCollecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.8)\n  Downloading typepy-1.3.4-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.8) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.8) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.8) (5.0.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.8) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.8) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.8) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.8) (1.18.3)\nRequirement already satisfied: chardet<6,>=3.0.4 in /usr/local/lib/python3.10/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm_eval==0.4.8) (5.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.26.0->lm_eval==0.4.8) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.26.0->lm_eval==0.4.8) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.26.0->lm_eval==0.4.8) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.26.0->lm_eval==0.4.8) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.26.0->lm_eval==0.4.8) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.26.0->lm_eval==0.4.8) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->lm_eval==0.4.8) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->lm_eval==0.4.8) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->lm_eval==0.4.8) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->lm_eval==0.4.8) (2025.1.31)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.8) (2.9.0.post0)\nRequirement already satisfied: pytz>=2018.9 in /usr/local/lib/python3.10/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.8) (2025.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8->lm_eval==0.4.8) (3.0.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->lm_eval==0.4.8) (2025.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate>=0.26.0->lm_eval==0.4.8) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate>=0.26.0->lm_eval==0.4.8) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate>=0.26.0->lm_eval==0.4.8) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate>=0.26.0->lm_eval==0.4.8) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3.0.0,>=1.17->accelerate>=0.26.0->lm_eval==0.4.8) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\nDownloading pytablewriter-1.2.1-py3-none-any.whl (91 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\nDownloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading DataProperty-1.1.0-py3-none-any.whl (27 kB)\nDownloading mbstrdecoder-1.1.4-py3-none-any.whl (7.9 kB)\nDownloading pathvalidate-3.2.3-py3-none-any.whl (24 kB)\nDownloading tabledata-1.3.4-py3-none-any.whl (11 kB)\nDownloading tcolorpy-0.1.7-py3-none-any.whl (8.1 kB)\nDownloading typepy-1.3.4-py3-none-any.whl (31 kB)\nDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\nBuilding wheels for collected packages: lm_eval, rouge-score, sqlitedict, word2number\n  Building editable for lm_eval (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for lm_eval: filename=lm_eval-0.4.8-0.editable-py3-none-any.whl size=24697 sha256=6f639070586864d4e4cdcc119d61308aa5da68b64579684215030df0275444f2\n  Stored in directory: /tmp/pip-ephem-wheel-cache-uuji9uj_/wheels/1b/1a/1b/44c80ddb18c9d7d3ce79a8d6d4561bddaddcbffb4cdfbf3259\n  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=bf3fda4c8cc8fd4f117071396189229a2f0d7e980b52105d8c8a63cb4e700566\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16864 sha256=73756bbda4baf143f73f030538c91722e307982fed1533d6db066c31e6032a09\n  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5568 sha256=8892780f61c69c74443120a58848c2c373ec8a68ab2852bd74fef9d49cb59cd5\n  Stored in directory: /root/.cache/pip/wheels/84/ff/26/d3cfbd971e96c5aa3737ecfced81628830d7359b55fbb8ca3b\nSuccessfully built lm_eval rouge-score sqlitedict word2number\nInstalling collected packages: word2number, sqlitedict, zstandard, tqdm-multiprocess, tcolorpy, portalocker, pathvalidate, mbstrdecoder, jsonlines, typepy, DataProperty, tabledata, pytablewriter, sacrebleu, rouge-score, evaluate, lm_eval\nSuccessfully installed DataProperty-1.1.0 evaluate-0.4.3 jsonlines-4.0.0 lm_eval-0.4.8 mbstrdecoder-1.1.4 pathvalidate-3.2.3 portalocker-3.1.1 pytablewriter-1.2.1 rouge-score-0.1.2 sacrebleu-2.5.1 sqlitedict-2.1.0 tabledata-1.3.4 tcolorpy-0.1.7 tqdm-multiprocess-0.0.11 typepy-1.3.4 word2number-1.1 zstandard-0.23.0\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"#benchmarks code for finetuned model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python -m lm_eval \\\n  --model hf \\\n  --model_args pretrained=sparky353454/last_latest_gemma_2_2b_it,revision=main,use_auth_token=True \\\n  --tasks gsm8k \\\n  --device cuda \\\n  --batch_size auto \\\n  --output_path results.json \\\n  --log_samples\n\n#run in console ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#hiridharian code for finetuned model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python -m lm_eval \\\n  --model hf \\\n  --model_args pretrained=sparky353454/last_latest_gemma_2_2b_it,revision=main,use_auth_token=True \\\n  --tasks gsm8k \\\n  --num_fewshot 1 \\\n  --device cuda:auto \\\n  --batch_size auto \\\n  --output_path ./HUMANEVAL_01.json \\\n  --confirm_run_unsafe_code","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#hiridharian code for base model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python -m lm_eval \\\n  --model hf \\\n  --model_args pretrained=google/gemma-2-2b-it,revision=main,use_auth_token=True \\\n  --tasks gsm8k \\\n  --num_fewshot 1 \\\n  --device cuda:auto \\\n  --batch_size auto \\\n  --output_path ./HUMANEVAL_01.json \\\n  --confirm_run_unsafe_code","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# old fine tuned benchmarks","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Tasks|Version|     Filter     |n-shot|  Metric   |   |Value |   |Stderr|\n|-----|------:|----------------|-----:|-----------|---|-----:|---|-----:|\n|gsm8k|      3|flexible-extract|     5|exact_match|↑  |0.2737|±  |0.0123|\n|     |       |strict-match    |     5|exact_match|↑  |0.2858|±  |0.0124|\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#time latency test ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndef load_model_and_tokenizer(model_name, use_auth_token=True):\n    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=use_auth_token)\n    model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=use_auth_token, torch_dtype=torch.float16)\n    model.to(\"cuda\")\n    model.eval()\n    return model, tokenizer\n\ndef measure_latency(model, tokenizer, prompt, n_trials=20, max_new_tokens=50):\n    times = []\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n\n    # Warm-up run\n    _ = model.generate(**inputs, max_new_tokens=max_new_tokens)\n\n    for _ in range(n_trials):\n        torch.cuda.synchronize()\n        start_time = time.time()\n        _ = model.generate(**inputs, max_new_tokens=max_new_tokens)\n        torch.cuda.synchronize()\n        elapsed = time.time() - start_time\n        times.append(elapsed)\n\n    avg_time = sum(times) / n_trials\n    return avg_time, times\n\n# Define models\nbase_model_name = \"google/gemma-2-2b-it\"\nfinetuned_model_name = \"sparky353454/last_latest_gemma_2_2b_it\"\nprompt = \"Solve the following problem: If you have 10 apples and give away 3, how many apples do you have?\"\n\n# Load models\nbase_model, base_tokenizer = load_model_and_tokenizer(base_model_name)\nfinetuned_model, finetuned_tokenizer = load_model_and_tokenizer(finetuned_model_name)\n\n# Measure latency\nbase_avg, base_times = measure_latency(base_model, base_tokenizer, prompt)\nfinetuned_avg, finetuned_times = measure_latency(finetuned_model, finetuned_tokenizer, prompt)\n\n# Output\nprint(f\"Base Model Average Latency: {base_avg:.4f} seconds\")\nprint(f\"Fine-Tuned Model Average Latency: {finetuned_avg:.4f} seconds\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T04:09:39.225247Z","iopub.execute_input":"2025-04-24T04:09:39.225698Z","iopub.status.idle":"2025-04-24T04:11:13.451187Z","shell.execute_reply.started":"2025-04-24T04:09:39.225667Z","shell.execute_reply":"2025-04-24T04:11:13.450262Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:810: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec6ddfdcb6194943adcbf621abf56972"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aafaf6f23b614c3d8354e72bbf1eef29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"056923d8c72441d5bb4f83d8b65da2c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ac871690df343d298999f24644c6c75"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71616b5602c3495aa5a0c56ac2cdb5f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d041d40df1db4fbf8e1d26124ac61826"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/peft/config.py:162: UserWarning: Unexpected keyword arguments ['alpha_pattern', 'bias', 'corda_config', 'eva_config', 'exclude_modules', 'fan_in_fan_out', 'init_lora_weights', 'layer_replication', 'layers_pattern', 'layers_to_transform', 'loftq_config', 'lora_alpha', 'lora_bias', 'lora_dropout', 'megatron_config', 'megatron_core', 'modules_to_save', 'r', 'rank_pattern', 'target_modules', 'trainable_token_indices', 'use_dora', 'use_rslora'] for class PeftConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n  warnings.warn(\nLoading adapter weights from sparky353454/last_latest_gemma_2_2b_it led to unexpected keys not found in the model: model.layers.0.self_attn.k_proj.lora_A.default.weight, model.layers.0.self_attn.k_proj.lora_B.default.weight, model.layers.0.self_attn.o_proj.lora_A.default.weight, model.layers.0.self_attn.o_proj.lora_B.default.weight, model.layers.0.mlp.gate_proj.lora_A.default.weight, model.layers.0.mlp.gate_proj.lora_B.default.weight, model.layers.0.mlp.up_proj.lora_A.default.weight, model.layers.0.mlp.up_proj.lora_B.default.weight, model.layers.0.mlp.down_proj.lora_A.default.weight, model.layers.0.mlp.down_proj.lora_B.default.weight, model.layers.1.self_attn.k_proj.lora_A.default.weight, model.layers.1.self_attn.k_proj.lora_B.default.weight, model.layers.1.self_attn.o_proj.lora_A.default.weight, model.layers.1.self_attn.o_proj.lora_B.default.weight, model.layers.1.mlp.gate_proj.lora_A.default.weight, model.layers.1.mlp.gate_proj.lora_B.default.weight, model.layers.1.mlp.up_proj.lora_A.default.weight, model.layers.1.mlp.up_proj.lora_B.default.weight, model.layers.1.mlp.down_proj.lora_A.default.weight, model.layers.1.mlp.down_proj.lora_B.default.weight, model.layers.2.self_attn.k_proj.lora_A.default.weight, model.layers.2.self_attn.k_proj.lora_B.default.weight, model.layers.2.self_attn.o_proj.lora_A.default.weight, model.layers.2.self_attn.o_proj.lora_B.default.weight, model.layers.2.mlp.gate_proj.lora_A.default.weight, model.layers.2.mlp.gate_proj.lora_B.default.weight, model.layers.2.mlp.up_proj.lora_A.default.weight, model.layers.2.mlp.up_proj.lora_B.default.weight, model.layers.2.mlp.down_proj.lora_A.default.weight, model.layers.2.mlp.down_proj.lora_B.default.weight, model.layers.3.self_attn.k_proj.lora_A.default.weight, model.layers.3.self_attn.k_proj.lora_B.default.weight, model.layers.3.self_attn.o_proj.lora_A.default.weight, model.layers.3.self_attn.o_proj.lora_B.default.weight, model.layers.3.mlp.gate_proj.lora_A.default.weight, model.layers.3.mlp.gate_proj.lora_B.default.weight, model.layers.3.mlp.up_proj.lora_A.default.weight, model.layers.3.mlp.up_proj.lora_B.default.weight, model.layers.3.mlp.down_proj.lora_A.default.weight, model.layers.3.mlp.down_proj.lora_B.default.weight, model.layers.4.self_attn.k_proj.lora_A.default.weight, model.layers.4.self_attn.k_proj.lora_B.default.weight, model.layers.4.self_attn.o_proj.lora_A.default.weight, model.layers.4.self_attn.o_proj.lora_B.default.weight, model.layers.4.mlp.gate_proj.lora_A.default.weight, model.layers.4.mlp.gate_proj.lora_B.default.weight, model.layers.4.mlp.up_proj.lora_A.default.weight, model.layers.4.mlp.up_proj.lora_B.default.weight, model.layers.4.mlp.down_proj.lora_A.default.weight, model.layers.4.mlp.down_proj.lora_B.default.weight, model.layers.5.self_attn.k_proj.lora_A.default.weight, model.layers.5.self_attn.k_proj.lora_B.default.weight, model.layers.5.self_attn.o_proj.lora_A.default.weight, model.layers.5.self_attn.o_proj.lora_B.default.weight, model.layers.5.mlp.gate_proj.lora_A.default.weight, model.layers.5.mlp.gate_proj.lora_B.default.weight, model.layers.5.mlp.up_proj.lora_A.default.weight, model.layers.5.mlp.up_proj.lora_B.default.weight, model.layers.5.mlp.down_proj.lora_A.default.weight, model.layers.5.mlp.down_proj.lora_B.default.weight, model.layers.6.self_attn.k_proj.lora_A.default.weight, model.layers.6.self_attn.k_proj.lora_B.default.weight, model.layers.6.self_attn.o_proj.lora_A.default.weight, model.layers.6.self_attn.o_proj.lora_B.default.weight, model.layers.6.mlp.gate_proj.lora_A.default.weight, model.layers.6.mlp.gate_proj.lora_B.default.weight, model.layers.6.mlp.up_proj.lora_A.default.weight, model.layers.6.mlp.up_proj.lora_B.default.weight, model.layers.6.mlp.down_proj.lora_A.default.weight, model.layers.6.mlp.down_proj.lora_B.default.weight, model.layers.7.self_attn.k_proj.lora_A.default.weight, model.layers.7.self_attn.k_proj.lora_B.default.weight, model.layers.7.self_attn.o_proj.lora_A.default.weight, model.layers.7.self_attn.o_proj.lora_B.default.weight, model.layers.7.mlp.gate_proj.lora_A.default.weight, model.layers.7.mlp.gate_proj.lora_B.default.weight, model.layers.7.mlp.up_proj.lora_A.default.weight, model.layers.7.mlp.up_proj.lora_B.default.weight, model.layers.7.mlp.down_proj.lora_A.default.weight, model.layers.7.mlp.down_proj.lora_B.default.weight, model.layers.8.self_attn.k_proj.lora_A.default.weight, model.layers.8.self_attn.k_proj.lora_B.default.weight, model.layers.8.self_attn.o_proj.lora_A.default.weight, model.layers.8.self_attn.o_proj.lora_B.default.weight, model.layers.8.mlp.gate_proj.lora_A.default.weight, model.layers.8.mlp.gate_proj.lora_B.default.weight, model.layers.8.mlp.up_proj.lora_A.default.weight, model.layers.8.mlp.up_proj.lora_B.default.weight, model.layers.8.mlp.down_proj.lora_A.default.weight, model.layers.8.mlp.down_proj.lora_B.default.weight, model.layers.9.self_attn.k_proj.lora_A.default.weight, model.layers.9.self_attn.k_proj.lora_B.default.weight, model.layers.9.self_attn.o_proj.lora_A.default.weight, model.layers.9.self_attn.o_proj.lora_B.default.weight, model.layers.9.mlp.gate_proj.lora_A.default.weight, model.layers.9.mlp.gate_proj.lora_B.default.weight, model.layers.9.mlp.up_proj.lora_A.default.weight, model.layers.9.mlp.up_proj.lora_B.default.weight, model.layers.9.mlp.down_proj.lora_A.default.weight, model.layers.9.mlp.down_proj.lora_B.default.weight, model.layers.10.self_attn.k_proj.lora_A.default.weight, model.layers.10.self_attn.k_proj.lora_B.default.weight, model.layers.10.self_attn.o_proj.lora_A.default.weight, model.layers.10.self_attn.o_proj.lora_B.default.weight, model.layers.10.mlp.gate_proj.lora_A.default.weight, model.layers.10.mlp.gate_proj.lora_B.default.weight, model.layers.10.mlp.up_proj.lora_A.default.weight, model.layers.10.mlp.up_proj.lora_B.default.weight, model.layers.10.mlp.down_proj.lora_A.default.weight, model.layers.10.mlp.down_proj.lora_B.default.weight, model.layers.11.self_attn.k_proj.lora_A.default.weight, model.layers.11.self_attn.k_proj.lora_B.default.weight, model.layers.11.self_attn.o_proj.lora_A.default.weight, model.layers.11.self_attn.o_proj.lora_B.default.weight, model.layers.11.mlp.gate_proj.lora_A.default.weight, model.layers.11.mlp.gate_proj.lora_B.default.weight, model.layers.11.mlp.up_proj.lora_A.default.weight, model.layers.11.mlp.up_proj.lora_B.default.weight, model.layers.11.mlp.down_proj.lora_A.default.weight, model.layers.11.mlp.down_proj.lora_B.default.weight, model.layers.12.self_attn.k_proj.lora_A.default.weight, model.layers.12.self_attn.k_proj.lora_B.default.weight, model.layers.12.self_attn.o_proj.lora_A.default.weight, model.layers.12.self_attn.o_proj.lora_B.default.weight, model.layers.12.mlp.gate_proj.lora_A.default.weight, model.layers.12.mlp.gate_proj.lora_B.default.weight, model.layers.12.mlp.up_proj.lora_A.default.weight, model.layers.12.mlp.up_proj.lora_B.default.weight, model.layers.12.mlp.down_proj.lora_A.default.weight, model.layers.12.mlp.down_proj.lora_B.default.weight, model.layers.13.self_attn.k_proj.lora_A.default.weight, model.layers.13.self_attn.k_proj.lora_B.default.weight, model.layers.13.self_attn.o_proj.lora_A.default.weight, model.layers.13.self_attn.o_proj.lora_B.default.weight, model.layers.13.mlp.gate_proj.lora_A.default.weight, model.layers.13.mlp.gate_proj.lora_B.default.weight, model.layers.13.mlp.up_proj.lora_A.default.weight, model.layers.13.mlp.up_proj.lora_B.default.weight, model.layers.13.mlp.down_proj.lora_A.default.weight, model.layers.13.mlp.down_proj.lora_B.default.weight, model.layers.14.self_attn.k_proj.lora_A.default.weight, model.layers.14.self_attn.k_proj.lora_B.default.weight, model.layers.14.self_attn.o_proj.lora_A.default.weight, model.layers.14.self_attn.o_proj.lora_B.default.weight, model.layers.14.mlp.gate_proj.lora_A.default.weight, model.layers.14.mlp.gate_proj.lora_B.default.weight, model.layers.14.mlp.up_proj.lora_A.default.weight, model.layers.14.mlp.up_proj.lora_B.default.weight, model.layers.14.mlp.down_proj.lora_A.default.weight, model.layers.14.mlp.down_proj.lora_B.default.weight, model.layers.15.self_attn.k_proj.lora_A.default.weight, model.layers.15.self_attn.k_proj.lora_B.default.weight, model.layers.15.self_attn.o_proj.lora_A.default.weight, model.layers.15.self_attn.o_proj.lora_B.default.weight, model.layers.15.mlp.gate_proj.lora_A.default.weight, model.layers.15.mlp.gate_proj.lora_B.default.weight, model.layers.15.mlp.up_proj.lora_A.default.weight, model.layers.15.mlp.up_proj.lora_B.default.weight, model.layers.15.mlp.down_proj.lora_A.default.weight, model.layers.15.mlp.down_proj.lora_B.default.weight, model.layers.16.self_attn.k_proj.lora_A.default.weight, model.layers.16.self_attn.k_proj.lora_B.default.weight, model.layers.16.self_attn.o_proj.lora_A.default.weight, model.layers.16.self_attn.o_proj.lora_B.default.weight, model.layers.16.mlp.gate_proj.lora_A.default.weight, model.layers.16.mlp.gate_proj.lora_B.default.weight, model.layers.16.mlp.up_proj.lora_A.default.weight, model.layers.16.mlp.up_proj.lora_B.default.weight, model.layers.16.mlp.down_proj.lora_A.default.weight, model.layers.16.mlp.down_proj.lora_B.default.weight, model.layers.17.self_attn.k_proj.lora_A.default.weight, model.layers.17.self_attn.k_proj.lora_B.default.weight, model.layers.17.self_attn.o_proj.lora_A.default.weight, model.layers.17.self_attn.o_proj.lora_B.default.weight, model.layers.17.mlp.gate_proj.lora_A.default.weight, model.layers.17.mlp.gate_proj.lora_B.default.weight, model.layers.17.mlp.up_proj.lora_A.default.weight, model.layers.17.mlp.up_proj.lora_B.default.weight, model.layers.17.mlp.down_proj.lora_A.default.weight, model.layers.17.mlp.down_proj.lora_B.default.weight, model.layers.18.self_attn.k_proj.lora_A.default.weight, model.layers.18.self_attn.k_proj.lora_B.default.weight, model.layers.18.self_attn.o_proj.lora_A.default.weight, model.layers.18.self_attn.o_proj.lora_B.default.weight, model.layers.18.mlp.gate_proj.lora_A.default.weight, model.layers.18.mlp.gate_proj.lora_B.default.weight, model.layers.18.mlp.up_proj.lora_A.default.weight, model.layers.18.mlp.up_proj.lora_B.default.weight, model.layers.18.mlp.down_proj.lora_A.default.weight, model.layers.18.mlp.down_proj.lora_B.default.weight, model.layers.19.self_attn.k_proj.lora_A.default.weight, model.layers.19.self_attn.k_proj.lora_B.default.weight, model.layers.19.self_attn.o_proj.lora_A.default.weight, model.layers.19.self_attn.o_proj.lora_B.default.weight, model.layers.19.mlp.gate_proj.lora_A.default.weight, model.layers.19.mlp.gate_proj.lora_B.default.weight, model.layers.19.mlp.up_proj.lora_A.default.weight, model.layers.19.mlp.up_proj.lora_B.default.weight, model.layers.19.mlp.down_proj.lora_A.default.weight, model.layers.19.mlp.down_proj.lora_B.default.weight, model.layers.20.self_attn.k_proj.lora_A.default.weight, model.layers.20.self_attn.k_proj.lora_B.default.weight, model.layers.20.self_attn.o_proj.lora_A.default.weight, model.layers.20.self_attn.o_proj.lora_B.default.weight, model.layers.20.mlp.gate_proj.lora_A.default.weight, model.layers.20.mlp.gate_proj.lora_B.default.weight, model.layers.20.mlp.up_proj.lora_A.default.weight, model.layers.20.mlp.up_proj.lora_B.default.weight, model.layers.20.mlp.down_proj.lora_A.default.weight, model.layers.20.mlp.down_proj.lora_B.default.weight, model.layers.21.self_attn.k_proj.lora_A.default.weight, model.layers.21.self_attn.k_proj.lora_B.default.weight, model.layers.21.self_attn.o_proj.lora_A.default.weight, model.layers.21.self_attn.o_proj.lora_B.default.weight, model.layers.21.mlp.gate_proj.lora_A.default.weight, model.layers.21.mlp.gate_proj.lora_B.default.weight, model.layers.21.mlp.up_proj.lora_A.default.weight, model.layers.21.mlp.up_proj.lora_B.default.weight, model.layers.21.mlp.down_proj.lora_A.default.weight, model.layers.21.mlp.down_proj.lora_B.default.weight, model.layers.22.self_attn.k_proj.lora_A.default.weight, model.layers.22.self_attn.k_proj.lora_B.default.weight, model.layers.22.self_attn.o_proj.lora_A.default.weight, model.layers.22.self_attn.o_proj.lora_B.default.weight, model.layers.22.mlp.gate_proj.lora_A.default.weight, model.layers.22.mlp.gate_proj.lora_B.default.weight, model.layers.22.mlp.up_proj.lora_A.default.weight, model.layers.22.mlp.up_proj.lora_B.default.weight, model.layers.22.mlp.down_proj.lora_A.default.weight, model.layers.22.mlp.down_proj.lora_B.default.weight, model.layers.23.self_attn.k_proj.lora_A.default.weight, model.layers.23.self_attn.k_proj.lora_B.default.weight, model.layers.23.self_attn.o_proj.lora_A.default.weight, model.layers.23.self_attn.o_proj.lora_B.default.weight, model.layers.23.mlp.gate_proj.lora_A.default.weight, model.layers.23.mlp.gate_proj.lora_B.default.weight, model.layers.23.mlp.up_proj.lora_A.default.weight, model.layers.23.mlp.up_proj.lora_B.default.weight, model.layers.23.mlp.down_proj.lora_A.default.weight, model.layers.23.mlp.down_proj.lora_B.default.weight, model.layers.24.self_attn.k_proj.lora_A.default.weight, model.layers.24.self_attn.k_proj.lora_B.default.weight, model.layers.24.self_attn.o_proj.lora_A.default.weight, model.layers.24.self_attn.o_proj.lora_B.default.weight, model.layers.24.mlp.gate_proj.lora_A.default.weight, model.layers.24.mlp.gate_proj.lora_B.default.weight, model.layers.24.mlp.up_proj.lora_A.default.weight, model.layers.24.mlp.up_proj.lora_B.default.weight, model.layers.24.mlp.down_proj.lora_A.default.weight, model.layers.24.mlp.down_proj.lora_B.default.weight, model.layers.25.self_attn.k_proj.lora_A.default.weight, model.layers.25.self_attn.k_proj.lora_B.default.weight, model.layers.25.self_attn.o_proj.lora_A.default.weight, model.layers.25.self_attn.o_proj.lora_B.default.weight, model.layers.25.mlp.gate_proj.lora_A.default.weight, model.layers.25.mlp.gate_proj.lora_B.default.weight, model.layers.25.mlp.up_proj.lora_A.default.weight, model.layers.25.mlp.up_proj.lora_B.default.weight, model.layers.25.mlp.down_proj.lora_A.default.weight, model.layers.25.mlp.down_proj.lora_B.default.weight. \nThe 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n","output_type":"stream"},{"name":"stdout","text":"Base Model Average Latency: 0.5616 seconds\nFine-Tuned Model Average Latency: 3.1158 seconds\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Base Model Average Latency: 0.5616 seconds\n# Fine-Tuned Model Average Latency: 3.1158 seconds","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#1 shot finetuned","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"hf (pretrained=sparky353454/last_latest_gemma_2_2b_it,revision=main,use_auth_token=True), gen_kwargs: (None), limit: None, num_fewshot: 1, batch_size: auto\n|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value |   |Stderr|\n|-----|------:|----------------|-----:|-----------|---|-----:|---|-----:|\n|gsm8k|      3|flexible-extract|     1|exact_match|↑  |0.5019|±  |0.0138|\n|     |       |strict-match    |     1|exact_match|↑  |0.0910|±  |0.0079|\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#5-shot finetuned","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"hf (pretrained=sparky353454/last_latest_gemma_2_2b_it,revision=main,use_auth_token=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto\n|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value |   |Stderr|\n|-----|------:|----------------|-----:|-----------|---|-----:|---|-----:|\n|gsm8k|      3|flexible-extract|     5|exact_match|↑  |0.4617|±  |0.0137|\n|     |       |strict-match    |     5|exact_match|↑  |0.4473|±  |0.0137|\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# base model benchmarks ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#5 -shot basemodel bench marks","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value |   |Stderr|\n|-----|------:|----------------|-----:|-----------|---|-----:|---|-----:|\n|gsm8k|      3|flexible-extract|     5|exact_match|↑  |0.4556|±  |0.0137|\n|     |       |strict-match    |     5|exact_match|↑  |0.4496|±  |0.0137|\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#1 shot benchmarking basemodel","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value |   |Stderr|\n|-----|------:|----------------|-----:|-----------|---|-----:|---|-----:|\n|gsm8k|      3|flexible-extract|     1|exact_match|↑  |0.5118|±  |0.0138|\n|     |       |strict-match    |     1|exact_match|↑  |0.1084|±  |0.0086|\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python -m lm_eval \\\n  --model hf \\\n  --model_args pretrained=google/gemma-2-2b-it,revision=main,use_auth_token=True \\\n  --tasks gsm8k \\\n  --num_fewshot 1 \\\n  --device cuda:auto \\\n  --batch_size auto \\\n  --output_path ./HUMANEVAL_01.json \\\n  --confirm_run_unsafe_code","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T06:45:05.999062Z","iopub.execute_input":"2025-04-24T06:45:05.999448Z","iopub.status.idle":"2025-04-24T09:48:00.029722Z","shell.execute_reply.started":"2025-04-24T06:45:05.999406Z","shell.execute_reply":"2025-04-24T09:48:00.028444Z"}},"outputs":[{"name":"stdout","text":"2025-04-24 06:45:10.142988: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-04-24 06:45:10.165794: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-04-24 06:45:10.172180: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nconfig.json: 100%|█████████████████████████████| 838/838 [00:00<00:00, 4.72MB/s]\ntokenizer_config.json: 100%|███████████████| 47.0k/47.0k [00:00<00:00, 4.36MB/s]\ntokenizer.model: 100%|█████████████████████| 4.24M/4.24M [00:00<00:00, 45.6MB/s]\ntokenizer.json: 100%|███████████████████████| 17.5M/17.5M [00:00<00:00, 214MB/s]\nspecial_tokens_map.json: 100%|█████████████████| 636/636 [00:00<00:00, 4.36MB/s]\n/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\nmodel.safetensors.index.json: 100%|████████| 24.2k/24.2k [00:00<00:00, 17.5MB/s]\nDownloading shards:   0%|                                 | 0/2 [00:00<?, ?it/s]\nmodel-00001-of-00002.safetensors:   0%|             | 0.00/4.99G [00:00<?, ?B/s]\u001b[A\nmodel-00001-of-00002.safetensors:   0%|    | 10.5M/4.99G [00:00<06:12, 13.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   1%|    | 52.4M/4.99G [00:00<01:05, 75.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   2%|     | 94.4M/4.99G [00:00<00:36, 135MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   3%|▏     | 126M/4.99G [00:01<00:40, 119MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   3%|▏     | 157M/4.99G [00:01<00:31, 151MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   4%|▏     | 199M/4.99G [00:01<00:29, 162MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   5%|▎     | 231M/4.99G [00:01<00:26, 181MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   6%|▎     | 294M/4.99G [00:01<00:18, 248MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   7%|▍     | 346M/4.99G [00:02<00:16, 286MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   8%|▍     | 388M/4.99G [00:02<00:14, 312MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   9%|▌     | 440M/4.99G [00:02<00:13, 345MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  10%|▌     | 482M/4.99G [00:02<00:13, 345MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  11%|▋     | 524M/4.99G [00:02<00:12, 348MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  12%|▋     | 577M/4.99G [00:02<00:11, 387MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  13%|▊     | 629M/4.99G [00:02<00:10, 409MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  14%|▊     | 682M/4.99G [00:02<00:12, 335MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  15%|▊     | 724M/4.99G [00:03<00:12, 342MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  16%|▉     | 776M/4.99G [00:03<00:11, 361MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  17%|█     | 849M/4.99G [00:03<00:10, 391MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  18%|█     | 902M/4.99G [00:03<00:11, 360MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  19%|█▏    | 944M/4.99G [00:03<00:11, 340MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  20%|█    | 1.02G/4.99G [00:03<00:09, 421MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  21%|█    | 1.07G/4.99G [00:03<00:08, 445MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  22%|█    | 1.12G/4.99G [00:03<00:08, 457MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  24%|█▏   | 1.17G/4.99G [00:04<00:09, 387MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  25%|█▏   | 1.23G/4.99G [00:04<00:09, 400MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  26%|█▎   | 1.28G/4.99G [00:04<00:09, 393MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  27%|█▎   | 1.36G/4.99G [00:04<00:07, 497MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  29%|█▍   | 1.43G/4.99G [00:04<00:06, 513MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  30%|█▍   | 1.49G/4.99G [00:04<00:07, 451MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  31%|█▌   | 1.56G/4.99G [00:04<00:07, 459MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  33%|█▋   | 1.65G/4.99G [00:05<00:07, 475MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  35%|█▋   | 1.73G/4.99G [00:05<00:06, 535MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  36%|█▊   | 1.79G/4.99G [00:05<00:06, 509MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  37%|█▊   | 1.86G/4.99G [00:05<00:05, 524MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  39%|█▉   | 1.93G/4.99G [00:05<00:05, 572MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  40%|██   | 2.01G/4.99G [00:05<00:04, 634MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  42%|██   | 2.10G/4.99G [00:05<00:04, 663MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  44%|██▏  | 2.17G/4.99G [00:06<00:04, 589MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  45%|██▏  | 2.23G/4.99G [00:06<00:04, 598MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  46%|██▎  | 2.32G/4.99G [00:06<00:04, 659MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  48%|██▍  | 2.39G/4.99G [00:06<00:04, 634MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  49%|██▍  | 2.46G/4.99G [00:06<00:03, 643MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  51%|██▌  | 2.54G/4.99G [00:06<00:04, 588MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  53%|██▋  | 2.63G/4.99G [00:06<00:03, 662MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  54%|██▋  | 2.71G/4.99G [00:06<00:04, 567MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  55%|██▊  | 2.77G/4.99G [00:07<00:04, 555MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  57%|██▊  | 2.83G/4.99G [00:07<00:04, 525MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  58%|██▉  | 2.89G/4.99G [00:07<00:03, 524MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  60%|██▉  | 2.99G/4.99G [00:07<00:03, 618MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  61%|███  | 3.06G/4.99G [00:07<00:03, 501MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  63%|███▏ | 3.12G/4.99G [00:07<00:04, 464MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  64%|███▏ | 3.20G/4.99G [00:07<00:03, 520MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  65%|███▎ | 3.26G/4.99G [00:08<00:03, 468MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  66%|███▎ | 3.31G/4.99G [00:08<00:03, 471MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  68%|███▍ | 3.38G/4.99G [00:08<00:03, 477MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  69%|███▍ | 3.46G/4.99G [00:08<00:02, 563MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  71%|███▌ | 3.52G/4.99G [00:08<00:03, 479MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  72%|███▌ | 3.58G/4.99G [00:08<00:03, 443MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  73%|███▋ | 3.66G/4.99G [00:08<00:02, 532MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  75%|███▋ | 3.72G/4.99G [00:08<00:02, 528MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  76%|███▊ | 3.79G/4.99G [00:09<00:02, 469MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  78%|███▉ | 3.88G/4.99G [00:09<00:01, 557MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  79%|███▉ | 3.95G/4.99G [00:09<00:01, 574MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  81%|████ | 4.02G/4.99G [00:09<00:01, 576MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  83%|████▏| 4.15G/4.99G [00:09<00:01, 766MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  86%|████▎| 4.30G/4.99G [00:09<00:00, 901MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  88%|████▍| 4.39G/4.99G [00:09<00:00, 876MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  90%|████▌| 4.51G/4.99G [00:09<00:00, 936MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  93%|███▋| 4.65G/4.99G [00:09<00:00, 1.03GB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  96%|███▊| 4.80G/4.99G [00:10<00:00, 1.17GB/s]\u001b[A\nmodel-00001-of-00002.safetensors: 100%|████▉| 4.99G/4.99G [00:10<00:00, 479MB/s]\u001b[A\nDownloading shards:  50%|████████████▌            | 1/2 [00:10<00:10, 10.50s/it]\nmodel-00002-of-00002.safetensors:   0%|              | 0.00/241M [00:00<?, ?B/s]\u001b[A\nmodel-00002-of-00002.safetensors:   4%|▏    | 10.5M/241M [00:00<00:08, 25.9MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  13%|▋    | 31.5M/241M [00:00<00:02, 70.2MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  44%|███    | 105M/241M [00:00<00:00, 240MB/s]\u001b[A\nmodel-00002-of-00002.safetensors: 100%|██████▉| 241M/241M [00:01<00:00, 201MB/s]\u001b[A\nDownloading shards: 100%|█████████████████████████| 2/2 [00:11<00:00,  5.92s/it]\nLoading checkpoint shards: 100%|██████████████████| 2/2 [00:02<00:00,  1.31s/it]\ngeneration_config.json: 100%|██████████████████| 187/187 [00:00<00:00, 1.31MB/s]\nREADME.md: 100%|███████████████████████████| 7.94k/7.94k [00:00<00:00, 41.7MB/s]\ntrain-00000-of-00001.parquet: 100%|████████| 2.31M/2.31M [00:00<00:00, 20.1MB/s]\ntest-00000-of-00001.parquet: 100%|███████████| 419k/419k [00:00<00:00, 37.7MB/s]\nGenerating train split: 100%|████| 7473/7473 [00:00<00:00, 118498.04 examples/s]\nGenerating test split: 100%|█████| 1319/1319 [00:00<00:00, 281899.97 examples/s]\n100%|██████████████████████████████████████| 1319/1319 [00:01<00:00, 762.20it/s]\nRunning generate_until requests:   0%|                 | 0/1319 [00:00<?, ?it/s]Passed argument batch_size = auto. Detecting largest batch size\nThe 'batch_size' argument of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'max_batch_size' argument instead.\nThe 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\nDetermined Largest batch size: 1\nRunning generate_until requests: 100%|████| 1319/1319 [3:02:12<00:00,  8.29s/it]\nhf (pretrained=google/gemma-2-2b-it,revision=main,use_auth_token=True), gen_kwargs: (None), limit: None, num_fewshot: 1, batch_size: auto\n|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value |   |Stderr|\n|-----|------:|----------------|-----:|-----------|---|-----:|---|-----:|\n|gsm8k|      3|flexible-extract|     1|exact_match|↑  |0.5118|±  |0.0138|\n|     |       |strict-match    |     1|exact_match|↑  |0.1084|±  |0.0086|\n\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}